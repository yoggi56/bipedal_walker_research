{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import ode\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch as th\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import TD3, SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "from bipedalWalkerPMTG import BipedalWalkerPMTG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to use gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "model_saved_file = \"./models/bipedWalkerHardcore_PMTG_SAC\"\n",
    "log_dir = \"./models/PMTG_SAC-bipedalWalkerHardcore-model\"\n",
    "best_model_dir = \"./models/PMTG_SAC-bipedalWalkerHardcore-model/best_single\"\n",
    "video_prefix = \"bipedWalkerHardcore_PMTG_SAC\"\n",
    "csv_addr = \"./statistics/bipedWalkerHardcore2d_PMTG_SAC_rewards.csv\"\n",
    "time_addr = \"./statistics/bipedWalkerHardcore2d_PMTG_SAC_elapcedTime.txt\"\n",
    "csv_path = \"./statistics/bipedWalkerHardcore2d_PMTG_SAC_rewards.csv\"\n",
    "time_path = \"./statistics/bipedWalkerHardcore2d_PMTG_SAC_elapcedTime.txt\"\n",
    "tb_log = \"./PMTGHardcore_tensorboard/\"\n",
    "tb_log_name = \"PMTG_SAC_exp1\"\n",
    "best_video_prefix = \"bipedWalkerHardcore_PMTG_SAC\"\n",
    "\n",
    "# Training Parameters\n",
    "if use_gpu == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    num_cpu = 1\n",
    "\n",
    "is_hard = True\n",
    "n_timesteps = int(1e7)\n",
    "\n",
    "seed = 2\n",
    "check_freq = 1000\n",
    "reward_threshold = 300\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 7.3e-4\n",
    "batch_size = 256\n",
    "gamma = 0.99 #\n",
    "tau = 0.01\n",
    "buffer_size = 2000000\n",
    "learning_starts = 10000\n",
    "noise_std = 0.1\n",
    "gradient_steps = 1\n",
    "ent_coef = 0.005\n",
    "train_freq = 1\n",
    "n_layers = 2\n",
    "n_neurons = 256\n",
    "\n",
    "activation = th.nn.modules.activation.ReLU\n",
    "net_arch = [400, 300]\n",
    "policy_kwargs = dict(activation_fn=activation, \n",
    "                    net_arch=net_arch)\n",
    "\n",
    "hyperparameters = {\n",
    "        #\"env_name\": env_name,\n",
    "        \"n_timesteps\": n_timesteps,\n",
    "        \"seed\": seed,\n",
    "        \"noise_std\": noise_std,\n",
    "\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"tau\": tau,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"policy_kwargs\": dict(\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# other params\n",
    "it = 0\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BipedalWalkerPMTG(is_hard=True, action_repeat=3, act_noise=0.3, rew_scale=5.0, learn=True)\n",
    "env = Monitor(env, log_dir)\n",
    "eval_env = BipedalWalkerPMTG(is_hard=True, action_repeat=1, act_noise=0.0, rew_scale=1.0, learn=False)\n",
    " \n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(policy='MlpPolicy', \n",
    "        env=env,\n",
    "        # action_noise=action_noise,\n",
    "        train_freq=train_freq,\n",
    "\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        tau=tau,\n",
    "        gamma=gamma,\n",
    "        buffer_size=buffer_size,\n",
    "        learning_starts=learning_starts,\n",
    "        gradient_steps=gradient_steps,\n",
    "        ent_coef=ent_coef,\n",
    "\n",
    "        policy_kwargs=policy_kwargs,\n",
    "\n",
    "        device=device,\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed\n",
      "_XSERVTransMakeAllCOTSServerListeners: server already running\n",
      "(EE) \n",
      "Fatal server error:\n",
      "(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \n"
     ]
    }
   ],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.mean_reward_lst = []\n",
    "        self.timesteps = []\n",
    "        self.s_reward = []\n",
    "        self.s_timestep = []\n",
    "        self.file_number = 0\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          \n",
    "          if len(x) > 0:\n",
    "            # Mean training reward over the last 100 episodes\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "            self.mean_reward_lst.append(mean_reward)\n",
    "            self.timesteps.append(self.num_timesteps)\n",
    "            episodes = len(y)\n",
    "\n",
    "            self.s_reward.append(mean_reward)\n",
    "            self.s_timestep.append(self.num_timesteps)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "              print(f\"Num timesteps: {self.num_timesteps}; Episodes: {episodes}\")\n",
    "              print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "              self.best_mean_reward = mean_reward\n",
    "              print(f\"Saving new best model to {self.save_path}...\")\n",
    "              self.model.save(self.save_path)\n",
    "\n",
    "            # 300-scores candidate test\n",
    "            if mean_reward > reward_threshold*4.2:\n",
    "              Yellow = \"\\033[0;33m\"\n",
    "              NC = \"\\033[0m\"\n",
    "              print(\"{0}Reward threshold achieved{1}\".format(Yellow, NC))\n",
    "              print(\"Evaluating model....\")\n",
    "              evals= evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True, render=False, callback=None,\n",
    "                              reward_threshold=None)\n",
    "              mean_reward_100 = np.mean(evals[0])\n",
    "              std_reward = np.mean(evals[1])\n",
    "              print(f\"Evaluation over 100 Episodes: {mean_reward_100} \")\n",
    "\n",
    "              if mean_reward_100 >= reward_threshold:\n",
    "                # create folder for best models\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "                best_dir = \"{0}/{1}\".format(best_model_dir, dt_string)\n",
    "                os.makedirs(best_dir, exist_ok=True)\n",
    "                # save model file\n",
    "                self.model.save(best_dir)\n",
    "                print(f\"Saving new best model to {self.save_path}...\")\n",
    "                # save file with hyperparameters, episodes number and reward\n",
    "                print(\"Saving training info...\")\n",
    "                filename = \"{0}/training_info.txt\".format(best_dir)\n",
    "                print(filename)\n",
    "                with open(filename, mode=\"w\") as f:\n",
    "                    f.write(\"Episodes: {0}\\r\\n\".format(episodes))\n",
    "                    f.write(\"Timesteps: {0}\\r\\n\".format(self.num_timesteps))\n",
    "                    f.write(\"Eval reward: {0}\\r\\n\".format(mean_reward_100))\n",
    "                    f.write(\"Info and Hyperparameters:\\r\\n\")\n",
    "                    for k, v in hyperparameters.items():\n",
    "                        str = \"    {0}: {1}\\r\\n\".format(k, v)\n",
    "                        f.write(str)\n",
    "                # save reward curve\n",
    "                print(\"Saving reward CSV-data...\")\n",
    "                filename = \"{0}/rewards.csv\".format(best_dir)\n",
    "                with open(filename, mode='w') as reward_file:\n",
    "                    reward_writer = csv.writer(reward_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "                    for i in range(len(self.s_timestep)):\n",
    "                        reward_writer.writerow([self.s_reward[i], self.s_timestep[i]])\n",
    "                print(f\"MISSION COMPLETED\")\n",
    "                print(f\"Score: {mean_reward_100}+/-{std_reward} reached at Episode: {episodes} \")\n",
    "                return False\n",
    "        if self.num_timesteps % 10000 == 0:\n",
    "          print(\"Saving new model file...\")\n",
    "          self.file_number += 1\n",
    "          filename = \"model{0}\".format(self.file_number)\n",
    "          nsave_path = os.path.join(log_dir, filename)\n",
    "          self.model.save(nsave_path)\n",
    "          \n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_mean_reward(self):\n",
    "      return self.mean_reward_lst, self.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000; Episodes: 50\n",
      "Best mean reward: -inf - Last mean reward per episode: -46.33\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 2000; Episodes: 97\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -48.79\n",
      "Num timesteps: 3000; Episodes: 141\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -50.33\n",
      "Num timesteps: 4000; Episodes: 183\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -50.09\n",
      "Num timesteps: 5000; Episodes: 228\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -51.13\n",
      "Num timesteps: 6000; Episodes: 280\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -50.76\n",
      "Num timesteps: 7000; Episodes: 325\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -54.11\n",
      "Num timesteps: 8000; Episodes: 376\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -53.73\n",
      "Num timesteps: 9000; Episodes: 424\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -46.77\n",
      "Num timesteps: 10000; Episodes: 474\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -46.89\n",
      "Saving new model file...\n",
      "Num timesteps: 11000; Episodes: 513\n",
      "Best mean reward: -46.33 - Last mean reward per episode: -42.56\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 12000; Episodes: 545\n",
      "Best mean reward: -42.56 - Last mean reward per episode: -35.83\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 13000; Episodes: 578\n",
      "Best mean reward: -35.83 - Last mean reward per episode: -23.40\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 14000; Episodes: 607\n",
      "Best mean reward: -23.40 - Last mean reward per episode: -15.16\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 15000; Episodes: 638\n",
      "Best mean reward: -15.16 - Last mean reward per episode: -6.91\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 16000; Episodes: 664\n",
      "Best mean reward: -6.91 - Last mean reward per episode: 0.53\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 17000; Episodes: 687\n",
      "Best mean reward: 0.53 - Last mean reward per episode: 5.89\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 18000; Episodes: 709\n",
      "Best mean reward: 5.89 - Last mean reward per episode: 12.98\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 19000; Episodes: 730\n",
      "Best mean reward: 12.98 - Last mean reward per episode: 22.07\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 20000; Episodes: 748\n",
      "Best mean reward: 22.07 - Last mean reward per episode: 25.66\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 21000; Episodes: 763\n",
      "Best mean reward: 25.66 - Last mean reward per episode: 31.39\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 22000; Episodes: 774\n",
      "Best mean reward: 31.39 - Last mean reward per episode: 31.62\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 23000; Episodes: 787\n",
      "Best mean reward: 31.62 - Last mean reward per episode: 37.29\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 24000; Episodes: 801\n",
      "Best mean reward: 37.29 - Last mean reward per episode: 40.53\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 25000; Episodes: 812\n",
      "Best mean reward: 40.53 - Last mean reward per episode: 47.81\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 26000; Episodes: 826\n",
      "Best mean reward: 47.81 - Last mean reward per episode: 50.66\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 27000; Episodes: 837\n",
      "Best mean reward: 50.66 - Last mean reward per episode: 53.24\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 28000; Episodes: 849\n",
      "Best mean reward: 53.24 - Last mean reward per episode: 54.26\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 29000; Episodes: 860\n",
      "Best mean reward: 54.26 - Last mean reward per episode: 53.27\n",
      "Num timesteps: 30000; Episodes: 871\n",
      "Best mean reward: 54.26 - Last mean reward per episode: 50.70\n",
      "Saving new model file...\n",
      "Num timesteps: 31000; Episodes: 878\n",
      "Best mean reward: 54.26 - Last mean reward per episode: 60.71\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 32000; Episodes: 884\n",
      "Best mean reward: 60.71 - Last mean reward per episode: 60.06\n",
      "Num timesteps: 33000; Episodes: 896\n",
      "Best mean reward: 60.71 - Last mean reward per episode: 56.31\n",
      "Num timesteps: 34000; Episodes: 907\n",
      "Best mean reward: 60.71 - Last mean reward per episode: 60.62\n",
      "Num timesteps: 35000; Episodes: 917\n",
      "Best mean reward: 60.71 - Last mean reward per episode: 52.99\n",
      "Num timesteps: 36000; Episodes: 927\n",
      "Best mean reward: 60.71 - Last mean reward per episode: 56.71\n",
      "Num timesteps: 37000; Episodes: 935\n",
      "Best mean reward: 60.71 - Last mean reward per episode: 59.27\n",
      "Num timesteps: 38000; Episodes: 942\n",
      "Best mean reward: 60.71 - Last mean reward per episode: 61.73\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 39000; Episodes: 952\n",
      "Best mean reward: 61.73 - Last mean reward per episode: 71.26\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 40000; Episodes: 962\n",
      "Best mean reward: 71.26 - Last mean reward per episode: 76.39\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 41000; Episodes: 973\n",
      "Best mean reward: 76.39 - Last mean reward per episode: 77.70\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 42000; Episodes: 982\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 76.91\n",
      "Num timesteps: 43000; Episodes: 993\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 76.17\n",
      "Num timesteps: 44000; Episodes: 1000\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 75.01\n",
      "Num timesteps: 45000; Episodes: 1011\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 66.70\n",
      "Num timesteps: 46000; Episodes: 1019\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 75.58\n",
      "Num timesteps: 47000; Episodes: 1028\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 75.68\n",
      "Num timesteps: 48000; Episodes: 1039\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 75.38\n",
      "Num timesteps: 49000; Episodes: 1048\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 73.21\n",
      "Num timesteps: 50000; Episodes: 1055\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 72.48\n",
      "Saving new model file...\n",
      "Num timesteps: 51000; Episodes: 1065\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 72.42\n",
      "Num timesteps: 52000; Episodes: 1076\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 68.54\n",
      "Num timesteps: 53000; Episodes: 1086\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 73.31\n",
      "Num timesteps: 54000; Episodes: 1095\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 76.38\n",
      "Num timesteps: 55000; Episodes: 1101\n",
      "Best mean reward: 77.70 - Last mean reward per episode: 78.46\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 56000; Episodes: 1111\n",
      "Best mean reward: 78.46 - Last mean reward per episode: 83.46\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 57000; Episodes: 1120\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 76.90\n",
      "Num timesteps: 58000; Episodes: 1129\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 75.00\n",
      "Num timesteps: 59000; Episodes: 1138\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 72.97\n",
      "Num timesteps: 60000; Episodes: 1145\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 66.65\n",
      "Saving new model file...\n",
      "Num timesteps: 61000; Episodes: 1158\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 62.88\n",
      "Num timesteps: 62000; Episodes: 1168\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 64.19\n",
      "Num timesteps: 63000; Episodes: 1176\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 65.96\n",
      "Num timesteps: 64000; Episodes: 1182\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 65.65\n",
      "Num timesteps: 65000; Episodes: 1193\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 69.80\n",
      "Num timesteps: 66000; Episodes: 1199\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 83.31\n",
      "Num timesteps: 67000; Episodes: 1209\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 82.83\n",
      "Num timesteps: 68000; Episodes: 1215\n",
      "Best mean reward: 83.46 - Last mean reward per episode: 85.15\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 69000; Episodes: 1224\n",
      "Best mean reward: 85.15 - Last mean reward per episode: 82.09\n",
      "Num timesteps: 70000; Episodes: 1233\n",
      "Best mean reward: 85.15 - Last mean reward per episode: 84.43\n",
      "Saving new model file...\n",
      "Num timesteps: 71000; Episodes: 1239\n",
      "Best mean reward: 85.15 - Last mean reward per episode: 91.60\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 72000; Episodes: 1249\n",
      "Best mean reward: 91.60 - Last mean reward per episode: 95.72\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 73000; Episodes: 1258\n",
      "Best mean reward: 95.72 - Last mean reward per episode: 101.48\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 74000; Episodes: 1264\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 98.58\n",
      "Num timesteps: 75000; Episodes: 1273\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 100.16\n",
      "Num timesteps: 76000; Episodes: 1283\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 98.73\n",
      "Num timesteps: 77000; Episodes: 1292\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 94.97\n",
      "Num timesteps: 78000; Episodes: 1299\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 83.32\n",
      "Num timesteps: 79000; Episodes: 1307\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 84.95\n",
      "Num timesteps: 80000; Episodes: 1317\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 86.39\n",
      "Saving new model file...\n",
      "Num timesteps: 81000; Episodes: 1325\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 91.26\n",
      "Num timesteps: 82000; Episodes: 1332\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 93.13\n",
      "Num timesteps: 83000; Episodes: 1338\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 85.11\n",
      "Num timesteps: 84000; Episodes: 1345\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 81.37\n",
      "Num timesteps: 85000; Episodes: 1355\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 74.17\n",
      "Num timesteps: 86000; Episodes: 1364\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 77.84\n",
      "Num timesteps: 87000; Episodes: 1371\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 72.82\n",
      "Num timesteps: 88000; Episodes: 1376\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 81.92\n",
      "Num timesteps: 89000; Episodes: 1384\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 83.13\n",
      "Num timesteps: 90000; Episodes: 1393\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 94.05\n",
      "Saving new model file...\n",
      "Num timesteps: 91000; Episodes: 1401\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 96.87\n",
      "Num timesteps: 92000; Episodes: 1409\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 99.64\n",
      "Num timesteps: 93000; Episodes: 1418\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 98.06\n",
      "Num timesteps: 94000; Episodes: 1425\n",
      "Best mean reward: 101.48 - Last mean reward per episode: 102.18\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 95000; Episodes: 1432\n",
      "Best mean reward: 102.18 - Last mean reward per episode: 103.53\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 96000; Episodes: 1440\n",
      "Best mean reward: 103.53 - Last mean reward per episode: 109.61\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 97000; Episodes: 1448\n",
      "Best mean reward: 109.61 - Last mean reward per episode: 117.93\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 98000; Episodes: 1458\n",
      "Best mean reward: 117.93 - Last mean reward per episode: 120.99\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 99000; Episodes: 1464\n",
      "Best mean reward: 120.99 - Last mean reward per episode: 120.20\n",
      "Num timesteps: 100000; Episodes: 1472\n",
      "Best mean reward: 120.99 - Last mean reward per episode: 121.83\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 101000; Episodes: 1479\n",
      "Best mean reward: 121.83 - Last mean reward per episode: 125.81\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 102000; Episodes: 1485\n",
      "Best mean reward: 125.81 - Last mean reward per episode: 127.20\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 103000; Episodes: 1490\n",
      "Best mean reward: 127.20 - Last mean reward per episode: 129.95\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 104000; Episodes: 1497\n",
      "Best mean reward: 129.95 - Last mean reward per episode: 127.29\n",
      "Num timesteps: 105000; Episodes: 1506\n",
      "Best mean reward: 129.95 - Last mean reward per episode: 125.31\n",
      "Num timesteps: 106000; Episodes: 1515\n",
      "Best mean reward: 129.95 - Last mean reward per episode: 129.68\n",
      "Num timesteps: 107000; Episodes: 1521\n",
      "Best mean reward: 129.95 - Last mean reward per episode: 133.91\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 108000; Episodes: 1525\n",
      "Best mean reward: 133.91 - Last mean reward per episode: 135.51\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 109000; Episodes: 1529\n",
      "Best mean reward: 135.51 - Last mean reward per episode: 150.77\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 110000; Episodes: 1537\n",
      "Best mean reward: 150.77 - Last mean reward per episode: 156.58\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 111000; Episodes: 1547\n",
      "Best mean reward: 156.58 - Last mean reward per episode: 153.41\n",
      "Num timesteps: 112000; Episodes: 1554\n",
      "Best mean reward: 156.58 - Last mean reward per episode: 151.63\n",
      "Num timesteps: 113000; Episodes: 1561\n",
      "Best mean reward: 156.58 - Last mean reward per episode: 157.54\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 114000; Episodes: 1570\n",
      "Best mean reward: 157.54 - Last mean reward per episode: 159.69\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 115000; Episodes: 1578\n",
      "Best mean reward: 159.69 - Last mean reward per episode: 162.08\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 116000; Episodes: 1584\n",
      "Best mean reward: 162.08 - Last mean reward per episode: 166.13\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 117000; Episodes: 1592\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 160.16\n",
      "Num timesteps: 118000; Episodes: 1600\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 157.02\n",
      "Num timesteps: 119000; Episodes: 1608\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 164.78\n",
      "Num timesteps: 120000; Episodes: 1618\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 160.70\n",
      "Saving new model file...\n",
      "Num timesteps: 121000; Episodes: 1624\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 161.08\n",
      "Num timesteps: 122000; Episodes: 1629\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 154.88\n",
      "Num timesteps: 123000; Episodes: 1637\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 148.73\n",
      "Num timesteps: 124000; Episodes: 1644\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 155.95\n",
      "Num timesteps: 125000; Episodes: 1652\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 159.83\n",
      "Num timesteps: 126000; Episodes: 1659\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 160.82\n",
      "Num timesteps: 127000; Episodes: 1666\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 161.30\n",
      "Num timesteps: 128000; Episodes: 1672\n",
      "Best mean reward: 166.13 - Last mean reward per episode: 172.98\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 129000; Episodes: 1678\n",
      "Best mean reward: 172.98 - Last mean reward per episode: 188.76\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 130000; Episodes: 1683\n",
      "Best mean reward: 188.76 - Last mean reward per episode: 188.72\n",
      "Saving new model file...\n",
      "Num timesteps: 131000; Episodes: 1693\n",
      "Best mean reward: 188.76 - Last mean reward per episode: 186.96\n",
      "Num timesteps: 132000; Episodes: 1698\n",
      "Best mean reward: 188.76 - Last mean reward per episode: 192.44\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 133000; Episodes: 1706\n",
      "Best mean reward: 192.44 - Last mean reward per episode: 190.44\n",
      "Num timesteps: 134000; Episodes: 1713\n",
      "Best mean reward: 192.44 - Last mean reward per episode: 194.10\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 135000; Episodes: 1719\n",
      "Best mean reward: 194.10 - Last mean reward per episode: 186.37\n",
      "Num timesteps: 136000; Episodes: 1726\n",
      "Best mean reward: 194.10 - Last mean reward per episode: 184.88\n",
      "Num timesteps: 137000; Episodes: 1733\n",
      "Best mean reward: 194.10 - Last mean reward per episode: 188.34\n",
      "Num timesteps: 138000; Episodes: 1738\n",
      "Best mean reward: 194.10 - Last mean reward per episode: 190.34\n",
      "Num timesteps: 139000; Episodes: 1744\n",
      "Best mean reward: 194.10 - Last mean reward per episode: 196.14\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 140000; Episodes: 1752\n",
      "Best mean reward: 196.14 - Last mean reward per episode: 193.91\n",
      "Saving new model file...\n",
      "Num timesteps: 141000; Episodes: 1757\n",
      "Best mean reward: 196.14 - Last mean reward per episode: 189.89\n",
      "Num timesteps: 142000; Episodes: 1764\n",
      "Best mean reward: 196.14 - Last mean reward per episode: 197.68\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 143000; Episodes: 1772\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 187.36\n",
      "Num timesteps: 144000; Episodes: 1780\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 173.12\n",
      "Num timesteps: 145000; Episodes: 1787\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 173.31\n",
      "Num timesteps: 146000; Episodes: 1792\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 175.65\n",
      "Num timesteps: 147000; Episodes: 1798\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 181.97\n",
      "Num timesteps: 148000; Episodes: 1805\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 181.13\n",
      "Num timesteps: 149000; Episodes: 1810\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 180.78\n",
      "Num timesteps: 150000; Episodes: 1816\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 192.01\n",
      "Saving new model file...\n",
      "Num timesteps: 151000; Episodes: 1821\n",
      "Best mean reward: 197.68 - Last mean reward per episode: 207.35\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 152000; Episodes: 1831\n",
      "Best mean reward: 207.35 - Last mean reward per episode: 207.16\n",
      "Num timesteps: 153000; Episodes: 1838\n",
      "Best mean reward: 207.35 - Last mean reward per episode: 203.47\n",
      "Num timesteps: 154000; Episodes: 1845\n",
      "Best mean reward: 207.35 - Last mean reward per episode: 201.96\n",
      "Num timesteps: 155000; Episodes: 1852\n",
      "Best mean reward: 207.35 - Last mean reward per episode: 203.15\n",
      "Num timesteps: 156000; Episodes: 1856\n",
      "Best mean reward: 207.35 - Last mean reward per episode: 212.48\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 157000; Episodes: 1862\n",
      "Best mean reward: 212.48 - Last mean reward per episode: 215.33\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 158000; Episodes: 1870\n",
      "Best mean reward: 215.33 - Last mean reward per episode: 217.93\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 159000; Episodes: 1877\n",
      "Best mean reward: 217.93 - Last mean reward per episode: 216.96\n",
      "Num timesteps: 160000; Episodes: 1886\n",
      "Best mean reward: 217.93 - Last mean reward per episode: 220.70\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 161000; Episodes: 1893\n",
      "Best mean reward: 220.70 - Last mean reward per episode: 227.56\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 162000; Episodes: 1898\n",
      "Best mean reward: 227.56 - Last mean reward per episode: 224.81\n",
      "Num timesteps: 163000; Episodes: 1905\n",
      "Best mean reward: 227.56 - Last mean reward per episode: 233.35\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 164000; Episodes: 1912\n",
      "Best mean reward: 233.35 - Last mean reward per episode: 228.49\n",
      "Num timesteps: 165000; Episodes: 1918\n",
      "Best mean reward: 233.35 - Last mean reward per episode: 227.42\n",
      "Num timesteps: 166000; Episodes: 1926\n",
      "Best mean reward: 233.35 - Last mean reward per episode: 225.11\n",
      "Num timesteps: 167000; Episodes: 1931\n",
      "Best mean reward: 233.35 - Last mean reward per episode: 233.58\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 168000; Episodes: 1935\n",
      "Best mean reward: 233.58 - Last mean reward per episode: 246.90\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 169000; Episodes: 1943\n",
      "Best mean reward: 246.90 - Last mean reward per episode: 241.97\n",
      "Num timesteps: 170000; Episodes: 1952\n",
      "Best mean reward: 246.90 - Last mean reward per episode: 243.24\n",
      "Saving new model file...\n",
      "Num timesteps: 171000; Episodes: 1957\n",
      "Best mean reward: 246.90 - Last mean reward per episode: 240.55\n",
      "Num timesteps: 172000; Episodes: 1963\n",
      "Best mean reward: 246.90 - Last mean reward per episode: 238.76\n",
      "Num timesteps: 173000; Episodes: 1967\n",
      "Best mean reward: 246.90 - Last mean reward per episode: 247.17\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 174000; Episodes: 1972\n",
      "Best mean reward: 247.17 - Last mean reward per episode: 256.12\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 175000; Episodes: 1977\n",
      "Best mean reward: 256.12 - Last mean reward per episode: 263.59\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 176000; Episodes: 1984\n",
      "Best mean reward: 263.59 - Last mean reward per episode: 259.48\n",
      "Num timesteps: 177000; Episodes: 1994\n",
      "Best mean reward: 263.59 - Last mean reward per episode: 256.41\n",
      "Num timesteps: 178000; Episodes: 2001\n",
      "Best mean reward: 263.59 - Last mean reward per episode: 245.84\n",
      "Num timesteps: 179000; Episodes: 2009\n",
      "Best mean reward: 263.59 - Last mean reward per episode: 253.53\n",
      "Num timesteps: 180000; Episodes: 2012\n",
      "Best mean reward: 263.59 - Last mean reward per episode: 264.13\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 181000; Episodes: 2020\n",
      "Best mean reward: 264.13 - Last mean reward per episode: 261.64\n",
      "Num timesteps: 182000; Episodes: 2025\n",
      "Best mean reward: 264.13 - Last mean reward per episode: 268.75\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 183000; Episodes: 2029\n",
      "Best mean reward: 268.75 - Last mean reward per episode: 268.07\n",
      "Num timesteps: 184000; Episodes: 2036\n",
      "Best mean reward: 268.75 - Last mean reward per episode: 258.80\n",
      "Num timesteps: 185000; Episodes: 2041\n",
      "Best mean reward: 268.75 - Last mean reward per episode: 254.99\n",
      "Num timesteps: 186000; Episodes: 2044\n",
      "Best mean reward: 268.75 - Last mean reward per episode: 265.13\n",
      "Num timesteps: 187000; Episodes: 2050\n",
      "Best mean reward: 268.75 - Last mean reward per episode: 287.04\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 188000; Episodes: 2057\n",
      "Best mean reward: 287.04 - Last mean reward per episode: 287.71\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 189000; Episodes: 2065\n",
      "Best mean reward: 287.71 - Last mean reward per episode: 284.95\n",
      "Num timesteps: 190000; Episodes: 2068\n",
      "Best mean reward: 287.71 - Last mean reward per episode: 274.83\n",
      "Saving new model file...\n",
      "Num timesteps: 191000; Episodes: 2076\n",
      "Best mean reward: 287.71 - Last mean reward per episode: 281.54\n",
      "Num timesteps: 192000; Episodes: 2083\n",
      "Best mean reward: 287.71 - Last mean reward per episode: 280.16\n",
      "Num timesteps: 193000; Episodes: 2088\n",
      "Best mean reward: 287.71 - Last mean reward per episode: 291.34\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 194000; Episodes: 2094\n",
      "Best mean reward: 291.34 - Last mean reward per episode: 303.75\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 195000; Episodes: 2100\n",
      "Best mean reward: 303.75 - Last mean reward per episode: 303.10\n",
      "Num timesteps: 196000; Episodes: 2107\n",
      "Best mean reward: 303.75 - Last mean reward per episode: 301.36\n",
      "Num timesteps: 197000; Episodes: 2112\n",
      "Best mean reward: 303.75 - Last mean reward per episode: 301.82\n",
      "Num timesteps: 198000; Episodes: 2117\n",
      "Best mean reward: 303.75 - Last mean reward per episode: 300.75\n",
      "Num timesteps: 199000; Episodes: 2123\n",
      "Best mean reward: 303.75 - Last mean reward per episode: 318.94\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 200000; Episodes: 2130\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 306.35\n",
      "Saving new model file...\n",
      "Num timesteps: 201000; Episodes: 2137\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 306.87\n",
      "Num timesteps: 202000; Episodes: 2138\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 311.12\n",
      "Num timesteps: 203000; Episodes: 2144\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 289.80\n",
      "Num timesteps: 204000; Episodes: 2149\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 279.90\n",
      "Num timesteps: 205000; Episodes: 2153\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 287.64\n",
      "Num timesteps: 206000; Episodes: 2160\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 293.18\n",
      "Num timesteps: 207000; Episodes: 2166\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 291.84\n",
      "Num timesteps: 208000; Episodes: 2173\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 291.89\n",
      "Num timesteps: 209000; Episodes: 2181\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 287.81\n",
      "Num timesteps: 210000; Episodes: 2187\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 282.30\n",
      "Saving new model file...\n",
      "Num timesteps: 211000; Episodes: 2190\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 283.02\n",
      "Num timesteps: 212000; Episodes: 2197\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 282.36\n",
      "Num timesteps: 213000; Episodes: 2201\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 280.06\n",
      "Num timesteps: 214000; Episodes: 2209\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 287.63\n",
      "Num timesteps: 215000; Episodes: 2217\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 283.10\n",
      "Num timesteps: 216000; Episodes: 2225\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 268.36\n",
      "Num timesteps: 217000; Episodes: 2229\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 278.34\n",
      "Num timesteps: 218000; Episodes: 2236\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 273.37\n",
      "Num timesteps: 219000; Episodes: 2240\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 293.89\n",
      "Num timesteps: 220000; Episodes: 2247\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 290.57\n",
      "Saving new model file...\n",
      "Num timesteps: 221000; Episodes: 2253\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 277.93\n",
      "Num timesteps: 222000; Episodes: 2259\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 278.07\n",
      "Num timesteps: 223000; Episodes: 2264\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 286.15\n",
      "Num timesteps: 224000; Episodes: 2268\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 297.21\n",
      "Num timesteps: 225000; Episodes: 2278\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 294.30\n",
      "Num timesteps: 226000; Episodes: 2286\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 299.36\n",
      "Num timesteps: 227000; Episodes: 2294\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 288.90\n",
      "Num timesteps: 228000; Episodes: 2301\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 290.69\n",
      "Num timesteps: 229000; Episodes: 2306\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 285.55\n",
      "Num timesteps: 230000; Episodes: 2313\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 284.70\n",
      "Saving new model file...\n",
      "Num timesteps: 231000; Episodes: 2318\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 291.81\n",
      "Num timesteps: 232000; Episodes: 2321\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 312.38\n",
      "Num timesteps: 233000; Episodes: 2328\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 308.97\n",
      "Num timesteps: 234000; Episodes: 2334\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 314.43\n",
      "Num timesteps: 235000; Episodes: 2340\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 302.27\n",
      "Num timesteps: 236000; Episodes: 2348\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 293.90\n",
      "Num timesteps: 237000; Episodes: 2355\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 296.47\n",
      "Num timesteps: 238000; Episodes: 2360\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 295.96\n",
      "Num timesteps: 239000; Episodes: 2365\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 295.55\n",
      "Num timesteps: 240000; Episodes: 2369\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 286.50\n",
      "Saving new model file...\n",
      "Num timesteps: 241000; Episodes: 2375\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 302.25\n",
      "Num timesteps: 242000; Episodes: 2380\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 312.92\n",
      "Num timesteps: 243000; Episodes: 2385\n",
      "Best mean reward: 318.94 - Last mean reward per episode: 320.69\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 244000; Episodes: 2391\n",
      "Best mean reward: 320.69 - Last mean reward per episode: 329.91\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 245000; Episodes: 2396\n",
      "Best mean reward: 329.91 - Last mean reward per episode: 330.61\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 246000; Episodes: 2401\n",
      "Best mean reward: 330.61 - Last mean reward per episode: 340.35\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 247000; Episodes: 2406\n",
      "Best mean reward: 340.35 - Last mean reward per episode: 341.88\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 248000; Episodes: 2413\n",
      "Best mean reward: 341.88 - Last mean reward per episode: 349.99\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 249000; Episodes: 2420\n",
      "Best mean reward: 349.99 - Last mean reward per episode: 333.07\n",
      "Num timesteps: 250000; Episodes: 2424\n",
      "Best mean reward: 349.99 - Last mean reward per episode: 339.94\n",
      "Saving new model file...\n",
      "Num timesteps: 251000; Episodes: 2432\n",
      "Best mean reward: 349.99 - Last mean reward per episode: 332.64\n",
      "Num timesteps: 252000; Episodes: 2437\n",
      "Best mean reward: 349.99 - Last mean reward per episode: 342.80\n",
      "Num timesteps: 253000; Episodes: 2440\n",
      "Best mean reward: 349.99 - Last mean reward per episode: 352.35\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 254000; Episodes: 2448\n",
      "Best mean reward: 352.35 - Last mean reward per episode: 360.88\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 255000; Episodes: 2453\n",
      "Best mean reward: 360.88 - Last mean reward per episode: 372.36\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 256000; Episodes: 2459\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 369.98\n",
      "Num timesteps: 257000; Episodes: 2466\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 355.55\n",
      "Num timesteps: 258000; Episodes: 2471\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 358.97\n",
      "Num timesteps: 259000; Episodes: 2476\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 361.25\n",
      "Num timesteps: 260000; Episodes: 2481\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 361.94\n",
      "Saving new model file...\n",
      "Num timesteps: 261000; Episodes: 2487\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 350.24\n",
      "Num timesteps: 262000; Episodes: 2492\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 362.46\n",
      "Num timesteps: 263000; Episodes: 2499\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 358.87\n",
      "Num timesteps: 264000; Episodes: 2503\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 359.17\n",
      "Num timesteps: 265000; Episodes: 2509\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 357.34\n",
      "Num timesteps: 266000; Episodes: 2515\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 365.97\n",
      "Num timesteps: 267000; Episodes: 2519\n",
      "Best mean reward: 372.36 - Last mean reward per episode: 380.99\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 268000; Episodes: 2521\n",
      "Best mean reward: 380.99 - Last mean reward per episode: 394.62\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 269000; Episodes: 2528\n",
      "Best mean reward: 394.62 - Last mean reward per episode: 394.52\n",
      "Num timesteps: 270000; Episodes: 2530\n",
      "Best mean reward: 394.62 - Last mean reward per episode: 412.75\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 271000; Episodes: 2537\n",
      "Best mean reward: 412.75 - Last mean reward per episode: 410.96\n",
      "Num timesteps: 272000; Episodes: 2542\n",
      "Best mean reward: 412.75 - Last mean reward per episode: 399.65\n",
      "Num timesteps: 273000; Episodes: 2547\n",
      "Best mean reward: 412.75 - Last mean reward per episode: 413.17\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 274000; Episodes: 2552\n",
      "Best mean reward: 413.17 - Last mean reward per episode: 410.14\n",
      "Num timesteps: 275000; Episodes: 2556\n",
      "Best mean reward: 413.17 - Last mean reward per episode: 417.13\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 276000; Episodes: 2562\n",
      "Best mean reward: 417.13 - Last mean reward per episode: 427.44\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 277000; Episodes: 2565\n",
      "Best mean reward: 427.44 - Last mean reward per episode: 442.55\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 278000; Episodes: 2569\n",
      "Best mean reward: 442.55 - Last mean reward per episode: 456.23\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 279000; Episodes: 2575\n",
      "Best mean reward: 456.23 - Last mean reward per episode: 440.20\n",
      "Num timesteps: 280000; Episodes: 2581\n",
      "Best mean reward: 456.23 - Last mean reward per episode: 436.72\n",
      "Saving new model file...\n",
      "Num timesteps: 281000; Episodes: 2586\n",
      "Best mean reward: 456.23 - Last mean reward per episode: 448.17\n",
      "Num timesteps: 282000; Episodes: 2588\n",
      "Best mean reward: 456.23 - Last mean reward per episode: 444.08\n",
      "Num timesteps: 283000; Episodes: 2597\n",
      "Best mean reward: 456.23 - Last mean reward per episode: 442.66\n",
      "Num timesteps: 284000; Episodes: 2599\n",
      "Best mean reward: 456.23 - Last mean reward per episode: 458.43\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 285000; Episodes: 2603\n",
      "Best mean reward: 458.43 - Last mean reward per episode: 460.26\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 286000; Episodes: 2608\n",
      "Best mean reward: 460.26 - Last mean reward per episode: 468.32\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 287000; Episodes: 2612\n",
      "Best mean reward: 468.32 - Last mean reward per episode: 476.46\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 288000; Episodes: 2614\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 476.06\n",
      "Num timesteps: 289000; Episodes: 2621\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 460.43\n",
      "Num timesteps: 290000; Episodes: 2629\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 438.61\n",
      "Saving new model file...\n",
      "Num timesteps: 291000; Episodes: 2632\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 437.89\n",
      "Num timesteps: 292000; Episodes: 2641\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 424.76\n",
      "Num timesteps: 293000; Episodes: 2644\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 431.88\n",
      "Num timesteps: 294000; Episodes: 2646\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 453.35\n",
      "Num timesteps: 295000; Episodes: 2648\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 473.68\n",
      "Num timesteps: 296000; Episodes: 2655\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 464.55\n",
      "Num timesteps: 297000; Episodes: 2659\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 460.08\n",
      "Num timesteps: 298000; Episodes: 2665\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 442.59\n",
      "Num timesteps: 299000; Episodes: 2669\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 439.78\n",
      "Num timesteps: 300000; Episodes: 2672\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 450.12\n",
      "Saving new model file...\n",
      "Num timesteps: 301000; Episodes: 2679\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 453.27\n",
      "Num timesteps: 302000; Episodes: 2683\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 455.20\n",
      "Num timesteps: 303000; Episodes: 2688\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 454.25\n",
      "Num timesteps: 304000; Episodes: 2692\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 451.09\n",
      "Num timesteps: 305000; Episodes: 2698\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 452.89\n",
      "Num timesteps: 306000; Episodes: 2703\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 445.17\n",
      "Num timesteps: 307000; Episodes: 2706\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 452.31\n",
      "Num timesteps: 308000; Episodes: 2711\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 445.83\n",
      "Num timesteps: 309000; Episodes: 2715\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 436.47\n",
      "Num timesteps: 310000; Episodes: 2720\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 443.51\n",
      "Saving new model file...\n",
      "Num timesteps: 311000; Episodes: 2725\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 455.70\n",
      "Num timesteps: 312000; Episodes: 2729\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 465.71\n",
      "Num timesteps: 313000; Episodes: 2735\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 466.28\n",
      "Num timesteps: 314000; Episodes: 2739\n",
      "Best mean reward: 476.46 - Last mean reward per episode: 481.31\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 315000; Episodes: 2747\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 437.55\n",
      "Num timesteps: 316000; Episodes: 2753\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 425.99\n",
      "Num timesteps: 317000; Episodes: 2760\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 418.49\n",
      "Num timesteps: 318000; Episodes: 2765\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 433.96\n",
      "Num timesteps: 319000; Episodes: 2771\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 428.75\n",
      "Num timesteps: 320000; Episodes: 2773\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 431.67\n",
      "Saving new model file...\n",
      "Num timesteps: 321000; Episodes: 2779\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 434.63\n",
      "Num timesteps: 322000; Episodes: 2785\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 424.96\n",
      "Num timesteps: 323000; Episodes: 2792\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 426.49\n",
      "Num timesteps: 324000; Episodes: 2797\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 424.07\n",
      "Num timesteps: 325000; Episodes: 2802\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 425.16\n",
      "Num timesteps: 326000; Episodes: 2806\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 431.16\n",
      "Num timesteps: 327000; Episodes: 2809\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 438.47\n",
      "Num timesteps: 328000; Episodes: 2814\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 436.28\n",
      "Num timesteps: 329000; Episodes: 2820\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 429.71\n",
      "Num timesteps: 330000; Episodes: 2827\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 417.76\n",
      "Saving new model file...\n",
      "Num timesteps: 331000; Episodes: 2831\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 421.72\n",
      "Num timesteps: 332000; Episodes: 2835\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 424.65\n",
      "Num timesteps: 333000; Episodes: 2842\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 420.60\n",
      "Num timesteps: 334000; Episodes: 2845\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 423.13\n",
      "Num timesteps: 335000; Episodes: 2851\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 441.12\n",
      "Num timesteps: 336000; Episodes: 2857\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 450.18\n",
      "Num timesteps: 337000; Episodes: 2861\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 440.90\n",
      "Num timesteps: 338000; Episodes: 2869\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 439.97\n",
      "Num timesteps: 339000; Episodes: 2873\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 438.61\n",
      "Num timesteps: 340000; Episodes: 2876\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 449.72\n",
      "Saving new model file...\n",
      "Num timesteps: 341000; Episodes: 2879\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 456.49\n",
      "Num timesteps: 342000; Episodes: 2885\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 465.32\n",
      "Num timesteps: 343000; Episodes: 2888\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 471.59\n",
      "Num timesteps: 344000; Episodes: 2891\n",
      "Best mean reward: 481.31 - Last mean reward per episode: 481.49\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 345000; Episodes: 2894\n",
      "Best mean reward: 481.49 - Last mean reward per episode: 487.41\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 346000; Episodes: 2897\n",
      "Best mean reward: 487.41 - Last mean reward per episode: 482.83\n",
      "Num timesteps: 347000; Episodes: 2900\n",
      "Best mean reward: 487.41 - Last mean reward per episode: 491.93\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 348000; Episodes: 2906\n",
      "Best mean reward: 491.93 - Last mean reward per episode: 487.74\n",
      "Num timesteps: 349000; Episodes: 2910\n",
      "Best mean reward: 491.93 - Last mean reward per episode: 482.76\n",
      "Num timesteps: 350000; Episodes: 2913\n",
      "Best mean reward: 491.93 - Last mean reward per episode: 495.97\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 351000; Episodes: 2917\n",
      "Best mean reward: 495.97 - Last mean reward per episode: 504.91\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 352000; Episodes: 2921\n",
      "Best mean reward: 504.91 - Last mean reward per episode: 502.95\n",
      "Num timesteps: 353000; Episodes: 2926\n",
      "Best mean reward: 504.91 - Last mean reward per episode: 508.91\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 354000; Episodes: 2930\n",
      "Best mean reward: 508.91 - Last mean reward per episode: 523.36\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 355000; Episodes: 2934\n",
      "Best mean reward: 523.36 - Last mean reward per episode: 520.06\n",
      "Num timesteps: 356000; Episodes: 2939\n",
      "Best mean reward: 523.36 - Last mean reward per episode: 526.06\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 357000; Episodes: 2944\n",
      "Best mean reward: 526.06 - Last mean reward per episode: 523.82\n",
      "Num timesteps: 358000; Episodes: 2949\n",
      "Best mean reward: 526.06 - Last mean reward per episode: 528.72\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 359000; Episodes: 2953\n",
      "Best mean reward: 528.72 - Last mean reward per episode: 548.27\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 360000; Episodes: 2957\n",
      "Best mean reward: 548.27 - Last mean reward per episode: 543.95\n",
      "Saving new model file...\n",
      "Num timesteps: 361000; Episodes: 2960\n",
      "Best mean reward: 548.27 - Last mean reward per episode: 559.72\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 362000; Episodes: 2966\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 550.39\n",
      "Num timesteps: 363000; Episodes: 2972\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 557.20\n",
      "Num timesteps: 364000; Episodes: 2976\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 539.40\n",
      "Num timesteps: 365000; Episodes: 2981\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 538.73\n",
      "Num timesteps: 366000; Episodes: 2985\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 539.69\n",
      "Num timesteps: 367000; Episodes: 2991\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 524.34\n",
      "Num timesteps: 368000; Episodes: 2997\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 520.98\n",
      "Num timesteps: 369000; Episodes: 3001\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 511.12\n",
      "Num timesteps: 370000; Episodes: 3006\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 512.37\n",
      "Saving new model file...\n",
      "Num timesteps: 371000; Episodes: 3011\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 510.01\n",
      "Num timesteps: 372000; Episodes: 3014\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 505.88\n",
      "Num timesteps: 373000; Episodes: 3021\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 505.04\n",
      "Num timesteps: 374000; Episodes: 3024\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 509.38\n",
      "Num timesteps: 375000; Episodes: 3028\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 507.26\n",
      "Num timesteps: 376000; Episodes: 3035\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 485.52\n",
      "Num timesteps: 377000; Episodes: 3038\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 496.46\n",
      "Num timesteps: 378000; Episodes: 3041\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 500.88\n",
      "Num timesteps: 379000; Episodes: 3046\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 488.54\n",
      "Num timesteps: 380000; Episodes: 3052\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 471.24\n",
      "Saving new model file...\n",
      "Num timesteps: 381000; Episodes: 3057\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 466.04\n",
      "Num timesteps: 382000; Episodes: 3061\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 470.67\n",
      "Num timesteps: 383000; Episodes: 3066\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 470.54\n",
      "Num timesteps: 384000; Episodes: 3071\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 470.84\n",
      "Num timesteps: 385000; Episodes: 3074\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 477.97\n",
      "Num timesteps: 386000; Episodes: 3078\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 489.43\n",
      "Num timesteps: 387000; Episodes: 3084\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 484.30\n",
      "Num timesteps: 388000; Episodes: 3089\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 477.85\n",
      "Num timesteps: 389000; Episodes: 3093\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 486.15\n",
      "Num timesteps: 390000; Episodes: 3098\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 478.41\n",
      "Saving new model file...\n",
      "Num timesteps: 391000; Episodes: 3102\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 487.26\n",
      "Num timesteps: 392000; Episodes: 3106\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 494.29\n",
      "Num timesteps: 393000; Episodes: 3112\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 480.67\n",
      "Num timesteps: 394000; Episodes: 3116\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 486.39\n",
      "Num timesteps: 395000; Episodes: 3122\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 474.23\n",
      "Num timesteps: 396000; Episodes: 3127\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 474.20\n",
      "Num timesteps: 397000; Episodes: 3132\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 471.70\n",
      "Num timesteps: 398000; Episodes: 3135\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 488.72\n",
      "Num timesteps: 399000; Episodes: 3139\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 495.40\n",
      "Num timesteps: 400000; Episodes: 3142\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 494.39\n",
      "Saving new model file...\n",
      "Num timesteps: 401000; Episodes: 3145\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 508.46\n",
      "Num timesteps: 402000; Episodes: 3150\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 519.85\n",
      "Num timesteps: 403000; Episodes: 3159\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 499.82\n",
      "Num timesteps: 404000; Episodes: 3162\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 498.52\n",
      "Num timesteps: 405000; Episodes: 3163\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 505.44\n",
      "Num timesteps: 406000; Episodes: 3165\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 501.05\n",
      "Num timesteps: 407000; Episodes: 3169\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 511.60\n",
      "Num timesteps: 408000; Episodes: 3174\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 498.96\n",
      "Num timesteps: 409000; Episodes: 3179\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 485.74\n",
      "Num timesteps: 410000; Episodes: 3185\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 487.63\n",
      "Saving new model file...\n",
      "Num timesteps: 411000; Episodes: 3189\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 497.48\n",
      "Num timesteps: 412000; Episodes: 3192\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 499.69\n",
      "Num timesteps: 413000; Episodes: 3197\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 501.38\n",
      "Num timesteps: 414000; Episodes: 3201\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 494.84\n",
      "Num timesteps: 415000; Episodes: 3205\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 503.89\n",
      "Num timesteps: 416000; Episodes: 3208\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 507.33\n",
      "Num timesteps: 417000; Episodes: 3214\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 501.71\n",
      "Num timesteps: 418000; Episodes: 3217\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 502.52\n",
      "Num timesteps: 419000; Episodes: 3220\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 519.19\n",
      "Num timesteps: 420000; Episodes: 3223\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 532.71\n",
      "Saving new model file...\n",
      "Num timesteps: 421000; Episodes: 3228\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 550.52\n",
      "Num timesteps: 422000; Episodes: 3231\n",
      "Best mean reward: 559.72 - Last mean reward per episode: 559.91\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 423000; Episodes: 3235\n",
      "Best mean reward: 559.91 - Last mean reward per episode: 563.41\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 424000; Episodes: 3239\n",
      "Best mean reward: 563.41 - Last mean reward per episode: 551.37\n",
      "Num timesteps: 425000; Episodes: 3244\n",
      "Best mean reward: 563.41 - Last mean reward per episode: 532.25\n",
      "Num timesteps: 426000; Episodes: 3248\n",
      "Best mean reward: 563.41 - Last mean reward per episode: 532.03\n",
      "Num timesteps: 427000; Episodes: 3251\n",
      "Best mean reward: 563.41 - Last mean reward per episode: 533.78\n",
      "Num timesteps: 428000; Episodes: 3255\n",
      "Best mean reward: 563.41 - Last mean reward per episode: 559.33\n",
      "Num timesteps: 429000; Episodes: 3258\n",
      "Best mean reward: 563.41 - Last mean reward per episode: 572.94\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 430000; Episodes: 3262\n",
      "Best mean reward: 572.94 - Last mean reward per episode: 574.17\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 431000; Episodes: 3266\n",
      "Best mean reward: 574.17 - Last mean reward per episode: 576.44\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 432000; Episodes: 3269\n",
      "Best mean reward: 576.44 - Last mean reward per episode: 580.01\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 433000; Episodes: 3274\n",
      "Best mean reward: 580.01 - Last mean reward per episode: 586.45\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 434000; Episodes: 3277\n",
      "Best mean reward: 586.45 - Last mean reward per episode: 598.22\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 435000; Episodes: 3281\n",
      "Best mean reward: 598.22 - Last mean reward per episode: 589.77\n",
      "Num timesteps: 436000; Episodes: 3287\n",
      "Best mean reward: 598.22 - Last mean reward per episode: 596.93\n",
      "Num timesteps: 437000; Episodes: 3290\n",
      "Best mean reward: 598.22 - Last mean reward per episode: 612.10\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 438000; Episodes: 3296\n",
      "Best mean reward: 612.10 - Last mean reward per episode: 600.71\n",
      "Num timesteps: 439000; Episodes: 3300\n",
      "Best mean reward: 612.10 - Last mean reward per episode: 590.92\n",
      "Num timesteps: 440000; Episodes: 3305\n",
      "Best mean reward: 612.10 - Last mean reward per episode: 592.21\n",
      "Saving new model file...\n",
      "Num timesteps: 441000; Episodes: 3310\n",
      "Best mean reward: 612.10 - Last mean reward per episode: 582.99\n",
      "Num timesteps: 442000; Episodes: 3314\n",
      "Best mean reward: 612.10 - Last mean reward per episode: 598.11\n",
      "Num timesteps: 443000; Episodes: 3316\n",
      "Best mean reward: 612.10 - Last mean reward per episode: 614.75\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 444000; Episodes: 3322\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 589.23\n",
      "Num timesteps: 445000; Episodes: 3326\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 579.25\n",
      "Num timesteps: 446000; Episodes: 3328\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 586.30\n",
      "Num timesteps: 447000; Episodes: 3332\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 569.73\n",
      "Num timesteps: 448000; Episodes: 3338\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 574.52\n",
      "Num timesteps: 449000; Episodes: 3341\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 594.09\n",
      "Num timesteps: 450000; Episodes: 3346\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 590.35\n",
      "Saving new model file...\n",
      "Num timesteps: 451000; Episodes: 3349\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 595.23\n",
      "Num timesteps: 452000; Episodes: 3353\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 582.79\n",
      "Num timesteps: 453000; Episodes: 3357\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 585.67\n",
      "Num timesteps: 454000; Episodes: 3362\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 584.21\n",
      "Num timesteps: 455000; Episodes: 3367\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 581.24\n",
      "Num timesteps: 456000; Episodes: 3369\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 583.93\n",
      "Num timesteps: 457000; Episodes: 3374\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 586.20\n",
      "Num timesteps: 458000; Episodes: 3379\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 575.23\n",
      "Num timesteps: 459000; Episodes: 3383\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 574.29\n",
      "Num timesteps: 460000; Episodes: 3388\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 582.50\n",
      "Saving new model file...\n",
      "Num timesteps: 461000; Episodes: 3392\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 582.17\n",
      "Num timesteps: 462000; Episodes: 3395\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 591.10\n",
      "Num timesteps: 463000; Episodes: 3397\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 605.75\n",
      "Num timesteps: 464000; Episodes: 3399\n",
      "Best mean reward: 614.75 - Last mean reward per episode: 615.92\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 465000; Episodes: 3403\n",
      "Best mean reward: 615.92 - Last mean reward per episode: 625.74\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 466000; Episodes: 3407\n",
      "Best mean reward: 625.74 - Last mean reward per episode: 624.62\n",
      "Num timesteps: 467000; Episodes: 3409\n",
      "Best mean reward: 625.74 - Last mean reward per episode: 648.71\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 468000; Episodes: 3413\n",
      "Best mean reward: 648.71 - Last mean reward per episode: 640.52\n",
      "Num timesteps: 469000; Episodes: 3417\n",
      "Best mean reward: 648.71 - Last mean reward per episode: 630.69\n",
      "Num timesteps: 470000; Episodes: 3421\n",
      "Best mean reward: 648.71 - Last mean reward per episode: 635.22\n",
      "Saving new model file...\n",
      "Num timesteps: 471000; Episodes: 3423\n",
      "Best mean reward: 648.71 - Last mean reward per episode: 643.20\n",
      "Num timesteps: 472000; Episodes: 3426\n",
      "Best mean reward: 648.71 - Last mean reward per episode: 655.50\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 473000; Episodes: 3430\n",
      "Best mean reward: 655.50 - Last mean reward per episode: 651.78\n",
      "Num timesteps: 474000; Episodes: 3437\n",
      "Best mean reward: 655.50 - Last mean reward per episode: 638.91\n",
      "Num timesteps: 475000; Episodes: 3439\n",
      "Best mean reward: 655.50 - Last mean reward per episode: 643.01\n",
      "Num timesteps: 476000; Episodes: 3441\n",
      "Best mean reward: 655.50 - Last mean reward per episode: 653.93\n",
      "Num timesteps: 477000; Episodes: 3445\n",
      "Best mean reward: 655.50 - Last mean reward per episode: 674.33\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 478000; Episodes: 3448\n",
      "Best mean reward: 674.33 - Last mean reward per episode: 684.28\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 479000; Episodes: 3453\n",
      "Best mean reward: 684.28 - Last mean reward per episode: 679.39\n",
      "Num timesteps: 480000; Episodes: 3455\n",
      "Best mean reward: 684.28 - Last mean reward per episode: 691.28\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 481000; Episodes: 3457\n",
      "Best mean reward: 691.28 - Last mean reward per episode: 703.54\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 482000; Episodes: 3461\n",
      "Best mean reward: 703.54 - Last mean reward per episode: 708.95\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 483000; Episodes: 3466\n",
      "Best mean reward: 708.95 - Last mean reward per episode: 704.17\n",
      "Num timesteps: 484000; Episodes: 3470\n",
      "Best mean reward: 708.95 - Last mean reward per episode: 704.52\n",
      "Num timesteps: 485000; Episodes: 3474\n",
      "Best mean reward: 708.95 - Last mean reward per episode: 696.26\n",
      "Num timesteps: 486000; Episodes: 3476\n",
      "Best mean reward: 708.95 - Last mean reward per episode: 710.13\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 487000; Episodes: 3482\n",
      "Best mean reward: 710.13 - Last mean reward per episode: 713.58\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 488000; Episodes: 3486\n",
      "Best mean reward: 713.58 - Last mean reward per episode: 724.97\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 489000; Episodes: 3489\n",
      "Best mean reward: 724.97 - Last mean reward per episode: 727.18\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 490000; Episodes: 3493\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 724.68\n",
      "Saving new model file...\n",
      "Num timesteps: 491000; Episodes: 3496\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 721.86\n",
      "Num timesteps: 492000; Episodes: 3500\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 710.45\n",
      "Num timesteps: 493000; Episodes: 3502\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 715.90\n",
      "Num timesteps: 494000; Episodes: 3505\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 721.55\n",
      "Num timesteps: 495000; Episodes: 3509\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 722.85\n",
      "Num timesteps: 496000; Episodes: 3513\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 722.48\n",
      "Num timesteps: 497000; Episodes: 3517\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 726.20\n",
      "Num timesteps: 498000; Episodes: 3521\n",
      "Best mean reward: 727.18 - Last mean reward per episode: 735.78\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 499000; Episodes: 3525\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 712.90\n",
      "Num timesteps: 500000; Episodes: 3531\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 693.43\n",
      "Saving new model file...\n",
      "Num timesteps: 501000; Episodes: 3534\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 717.52\n",
      "Num timesteps: 502000; Episodes: 3539\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 701.50\n",
      "Num timesteps: 503000; Episodes: 3542\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 686.56\n",
      "Num timesteps: 504000; Episodes: 3545\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 691.18\n",
      "Num timesteps: 505000; Episodes: 3548\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 680.98\n",
      "Num timesteps: 506000; Episodes: 3552\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 679.88\n",
      "Num timesteps: 507000; Episodes: 3557\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 657.65\n",
      "Num timesteps: 508000; Episodes: 3560\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 669.12\n",
      "Num timesteps: 509000; Episodes: 3563\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 677.50\n",
      "Num timesteps: 510000; Episodes: 3568\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 680.32\n",
      "Saving new model file...\n",
      "Num timesteps: 511000; Episodes: 3571\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 684.22\n",
      "Num timesteps: 512000; Episodes: 3574\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 700.50\n",
      "Num timesteps: 513000; Episodes: 3578\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 690.74\n",
      "Num timesteps: 514000; Episodes: 3580\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 703.88\n",
      "Num timesteps: 515000; Episodes: 3586\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 689.93\n",
      "Num timesteps: 516000; Episodes: 3590\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 693.39\n",
      "Num timesteps: 517000; Episodes: 3592\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 695.35\n",
      "Num timesteps: 518000; Episodes: 3596\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 702.87\n",
      "Num timesteps: 519000; Episodes: 3599\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 703.64\n",
      "Num timesteps: 520000; Episodes: 3601\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 700.62\n",
      "Saving new model file...\n",
      "Num timesteps: 521000; Episodes: 3604\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 712.39\n",
      "Num timesteps: 522000; Episodes: 3607\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 710.34\n",
      "Num timesteps: 523000; Episodes: 3610\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 708.50\n",
      "Num timesteps: 524000; Episodes: 3613\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 733.93\n",
      "Num timesteps: 525000; Episodes: 3616\n",
      "Best mean reward: 735.78 - Last mean reward per episode: 743.25\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 526000; Episodes: 3619\n",
      "Best mean reward: 743.25 - Last mean reward per episode: 748.49\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 527000; Episodes: 3622\n",
      "Best mean reward: 748.49 - Last mean reward per episode: 749.77\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 528000; Episodes: 3625\n",
      "Best mean reward: 749.77 - Last mean reward per episode: 758.90\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 529000; Episodes: 3629\n",
      "Best mean reward: 758.90 - Last mean reward per episode: 771.36\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 530000; Episodes: 3634\n",
      "Best mean reward: 771.36 - Last mean reward per episode: 757.19\n",
      "Saving new model file...\n",
      "Num timesteps: 531000; Episodes: 3638\n",
      "Best mean reward: 771.36 - Last mean reward per episode: 767.80\n",
      "Num timesteps: 532000; Episodes: 3641\n",
      "Best mean reward: 771.36 - Last mean reward per episode: 756.26\n",
      "Num timesteps: 533000; Episodes: 3644\n",
      "Best mean reward: 771.36 - Last mean reward per episode: 769.40\n",
      "Num timesteps: 534000; Episodes: 3648\n",
      "Best mean reward: 771.36 - Last mean reward per episode: 776.22\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 535000; Episodes: 3652\n",
      "Best mean reward: 776.22 - Last mean reward per episode: 773.29\n",
      "Num timesteps: 536000; Episodes: 3656\n",
      "Best mean reward: 776.22 - Last mean reward per episode: 777.18\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 537000; Episodes: 3659\n",
      "Best mean reward: 777.18 - Last mean reward per episode: 775.72\n",
      "Num timesteps: 538000; Episodes: 3661\n",
      "Best mean reward: 777.18 - Last mean reward per episode: 774.10\n",
      "Num timesteps: 539000; Episodes: 3664\n",
      "Best mean reward: 777.18 - Last mean reward per episode: 782.46\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 540000; Episodes: 3668\n",
      "Best mean reward: 782.46 - Last mean reward per episode: 784.05\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 541000; Episodes: 3672\n",
      "Best mean reward: 784.05 - Last mean reward per episode: 772.96\n",
      "Num timesteps: 542000; Episodes: 3674\n",
      "Best mean reward: 784.05 - Last mean reward per episode: 785.33\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 543000; Episodes: 3676\n",
      "Best mean reward: 785.33 - Last mean reward per episode: 789.47\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 544000; Episodes: 3680\n",
      "Best mean reward: 789.47 - Last mean reward per episode: 792.88\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 545000; Episodes: 3682\n",
      "Best mean reward: 792.88 - Last mean reward per episode: 797.17\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 546000; Episodes: 3686\n",
      "Best mean reward: 797.17 - Last mean reward per episode: 814.48\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 547000; Episodes: 3690\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 805.99\n",
      "Num timesteps: 548000; Episodes: 3698\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 757.16\n",
      "Num timesteps: 549000; Episodes: 3702\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 749.78\n",
      "Num timesteps: 550000; Episodes: 3705\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 746.29\n",
      "Saving new model file...\n",
      "Num timesteps: 551000; Episodes: 3710\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 729.06\n",
      "Num timesteps: 552000; Episodes: 3712\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 724.25\n",
      "Num timesteps: 553000; Episodes: 3715\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 735.06\n",
      "Num timesteps: 554000; Episodes: 3719\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 727.75\n",
      "Num timesteps: 555000; Episodes: 3722\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 731.55\n",
      "Num timesteps: 556000; Episodes: 3725\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 725.18\n",
      "Num timesteps: 557000; Episodes: 3730\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 731.37\n",
      "Num timesteps: 558000; Episodes: 3735\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 731.51\n",
      "Num timesteps: 559000; Episodes: 3740\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 723.29\n",
      "Num timesteps: 560000; Episodes: 3742\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 723.56\n",
      "Saving new model file...\n",
      "Num timesteps: 561000; Episodes: 3745\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 723.29\n",
      "Num timesteps: 562000; Episodes: 3747\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 738.59\n",
      "Num timesteps: 563000; Episodes: 3751\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 739.27\n",
      "Num timesteps: 564000; Episodes: 3755\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 740.70\n",
      "Num timesteps: 565000; Episodes: 3759\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 748.47\n",
      "Num timesteps: 566000; Episodes: 3763\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 727.63\n",
      "Num timesteps: 567000; Episodes: 3766\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 716.82\n",
      "Num timesteps: 568000; Episodes: 3771\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 727.90\n",
      "Num timesteps: 569000; Episodes: 3774\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 710.37\n",
      "Num timesteps: 570000; Episodes: 3777\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 703.68\n",
      "Saving new model file...\n",
      "Num timesteps: 571000; Episodes: 3780\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 704.64\n",
      "Num timesteps: 572000; Episodes: 3782\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 707.33\n",
      "Num timesteps: 573000; Episodes: 3785\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 708.85\n",
      "Num timesteps: 574000; Episodes: 3789\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 711.06\n",
      "Num timesteps: 575000; Episodes: 3793\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 732.18\n",
      "Num timesteps: 576000; Episodes: 3797\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 750.83\n",
      "Num timesteps: 577000; Episodes: 3799\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 758.67\n",
      "Num timesteps: 578000; Episodes: 3803\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 762.86\n",
      "Num timesteps: 579000; Episodes: 3805\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 767.59\n",
      "Num timesteps: 580000; Episodes: 3810\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 772.54\n",
      "Saving new model file...\n",
      "Num timesteps: 581000; Episodes: 3813\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 764.48\n",
      "Num timesteps: 582000; Episodes: 3816\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 764.65\n",
      "Num timesteps: 583000; Episodes: 3818\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 785.98\n",
      "Num timesteps: 584000; Episodes: 3820\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 786.70\n",
      "Num timesteps: 585000; Episodes: 3822\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 798.56\n",
      "Num timesteps: 586000; Episodes: 3826\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 789.88\n",
      "Num timesteps: 587000; Episodes: 3828\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 804.07\n",
      "Num timesteps: 588000; Episodes: 3832\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 795.36\n",
      "Num timesteps: 589000; Episodes: 3836\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 801.54\n",
      "Num timesteps: 590000; Episodes: 3839\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 813.57\n",
      "Saving new model file...\n",
      "Num timesteps: 591000; Episodes: 3843\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 810.47\n",
      "Num timesteps: 592000; Episodes: 3846\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 808.36\n",
      "Num timesteps: 593000; Episodes: 3848\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 805.76\n",
      "Num timesteps: 594000; Episodes: 3851\n",
      "Best mean reward: 814.48 - Last mean reward per episode: 822.25\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 595000; Episodes: 3855\n",
      "Best mean reward: 822.25 - Last mean reward per episode: 807.20\n",
      "Num timesteps: 596000; Episodes: 3857\n",
      "Best mean reward: 822.25 - Last mean reward per episode: 819.67\n",
      "Num timesteps: 597000; Episodes: 3861\n",
      "Best mean reward: 822.25 - Last mean reward per episode: 803.91\n",
      "Num timesteps: 598000; Episodes: 3864\n",
      "Best mean reward: 822.25 - Last mean reward per episode: 818.30\n",
      "Num timesteps: 599000; Episodes: 3868\n",
      "Best mean reward: 822.25 - Last mean reward per episode: 829.27\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 600000; Episodes: 3870\n",
      "Best mean reward: 829.27 - Last mean reward per episode: 853.35\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 601000; Episodes: 3872\n",
      "Best mean reward: 853.35 - Last mean reward per episode: 862.20\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 602000; Episodes: 3875\n",
      "Best mean reward: 862.20 - Last mean reward per episode: 860.28\n",
      "Num timesteps: 603000; Episodes: 3878\n",
      "Best mean reward: 862.20 - Last mean reward per episode: 868.28\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 604000; Episodes: 3881\n",
      "Best mean reward: 868.28 - Last mean reward per episode: 867.20\n",
      "Num timesteps: 605000; Episodes: 3883\n",
      "Best mean reward: 868.28 - Last mean reward per episode: 871.48\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 606000; Episodes: 3886\n",
      "Best mean reward: 871.48 - Last mean reward per episode: 875.63\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 607000; Episodes: 3889\n",
      "Best mean reward: 875.63 - Last mean reward per episode: 878.42\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 608000; Episodes: 3892\n",
      "Best mean reward: 878.42 - Last mean reward per episode: 887.60\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 609000; Episodes: 3894\n",
      "Best mean reward: 887.60 - Last mean reward per episode: 899.12\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 610000; Episodes: 3897\n",
      "Best mean reward: 899.12 - Last mean reward per episode: 908.40\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 611000; Episodes: 3902\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 884.12\n",
      "Num timesteps: 612000; Episodes: 3905\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 884.00\n",
      "Num timesteps: 613000; Episodes: 3908\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 881.94\n",
      "Num timesteps: 614000; Episodes: 3911\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 887.58\n",
      "Num timesteps: 615000; Episodes: 3913\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 904.10\n",
      "Num timesteps: 616000; Episodes: 3916\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 904.71\n",
      "Num timesteps: 617000; Episodes: 3919\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 889.29\n",
      "Num timesteps: 618000; Episodes: 3921\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 884.49\n",
      "Num timesteps: 619000; Episodes: 3925\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 870.78\n",
      "Num timesteps: 620000; Episodes: 3932\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 862.40\n",
      "Saving new model file...\n",
      "Num timesteps: 621000; Episodes: 3934\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 881.96\n",
      "Num timesteps: 622000; Episodes: 3936\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 887.94\n",
      "Num timesteps: 623000; Episodes: 3941\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 870.26\n",
      "Num timesteps: 624000; Episodes: 3944\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 873.69\n",
      "Num timesteps: 625000; Episodes: 3947\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 882.88\n",
      "Num timesteps: 626000; Episodes: 3949\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 885.33\n",
      "Num timesteps: 627000; Episodes: 3953\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 889.70\n",
      "Num timesteps: 628000; Episodes: 3955\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 891.81\n",
      "Num timesteps: 629000; Episodes: 3959\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 888.38\n",
      "Num timesteps: 630000; Episodes: 3963\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 876.79\n",
      "Saving new model file...\n",
      "Num timesteps: 631000; Episodes: 3967\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 884.20\n",
      "Num timesteps: 632000; Episodes: 3970\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 872.88\n",
      "Num timesteps: 633000; Episodes: 3973\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 873.58\n",
      "Num timesteps: 634000; Episodes: 3976\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 873.75\n",
      "Num timesteps: 635000; Episodes: 3979\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 875.89\n",
      "Num timesteps: 636000; Episodes: 3982\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 864.47\n",
      "Num timesteps: 637000; Episodes: 3984\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 867.44\n",
      "Num timesteps: 638000; Episodes: 3988\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 868.22\n",
      "Num timesteps: 639000; Episodes: 3994\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 828.74\n",
      "Num timesteps: 640000; Episodes: 3998\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 811.78\n",
      "Saving new model file...\n",
      "Num timesteps: 641000; Episodes: 4001\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 820.52\n",
      "Num timesteps: 642000; Episodes: 4004\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 835.84\n",
      "Num timesteps: 643000; Episodes: 4006\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 833.39\n",
      "Num timesteps: 644000; Episodes: 4009\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 841.16\n",
      "Num timesteps: 645000; Episodes: 4011\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 849.36\n",
      "Num timesteps: 646000; Episodes: 4014\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 853.99\n",
      "Num timesteps: 647000; Episodes: 4016\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 855.47\n",
      "Num timesteps: 648000; Episodes: 4019\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 856.27\n",
      "Num timesteps: 649000; Episodes: 4023\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 852.72\n",
      "Num timesteps: 650000; Episodes: 4026\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 851.89\n",
      "Saving new model file...\n",
      "Num timesteps: 651000; Episodes: 4028\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 868.94\n",
      "Num timesteps: 652000; Episodes: 4032\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 881.65\n",
      "Num timesteps: 653000; Episodes: 4036\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 861.03\n",
      "Num timesteps: 654000; Episodes: 4039\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 873.40\n",
      "Num timesteps: 655000; Episodes: 4041\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 880.81\n",
      "Num timesteps: 656000; Episodes: 4047\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 847.64\n",
      "Num timesteps: 657000; Episodes: 4050\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 842.94\n",
      "Num timesteps: 658000; Episodes: 4052\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 849.79\n",
      "Num timesteps: 659000; Episodes: 4055\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 860.58\n",
      "Num timesteps: 660000; Episodes: 4058\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 853.11\n",
      "Saving new model file...\n",
      "Num timesteps: 661000; Episodes: 4061\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 868.87\n",
      "Num timesteps: 662000; Episodes: 4063\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 888.47\n",
      "Num timesteps: 663000; Episodes: 4065\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 894.83\n",
      "Num timesteps: 664000; Episodes: 4069\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 885.60\n",
      "Num timesteps: 665000; Episodes: 4072\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 886.48\n",
      "Num timesteps: 666000; Episodes: 4075\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 882.69\n",
      "Num timesteps: 667000; Episodes: 4077\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 897.64\n",
      "Num timesteps: 668000; Episodes: 4079\n",
      "Best mean reward: 908.40 - Last mean reward per episode: 909.77\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 669000; Episodes: 4082\n",
      "Best mean reward: 909.77 - Last mean reward per episode: 910.16\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 670000; Episodes: 4085\n",
      "Best mean reward: 910.16 - Last mean reward per episode: 906.96\n",
      "Saving new model file...\n",
      "Num timesteps: 671000; Episodes: 4087\n",
      "Best mean reward: 910.16 - Last mean reward per episode: 918.27\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 672000; Episodes: 4089\n",
      "Best mean reward: 918.27 - Last mean reward per episode: 935.26\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 673000; Episodes: 4091\n",
      "Best mean reward: 935.26 - Last mean reward per episode: 946.95\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 674000; Episodes: 4094\n",
      "Best mean reward: 946.95 - Last mean reward per episode: 975.05\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 675000; Episodes: 4097\n",
      "Best mean reward: 975.05 - Last mean reward per episode: 987.54\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 676000; Episodes: 4100\n",
      "Best mean reward: 987.54 - Last mean reward per episode: 992.72\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 677000; Episodes: 4104\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 979.95\n",
      "Num timesteps: 678000; Episodes: 4107\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 971.80\n",
      "Num timesteps: 679000; Episodes: 4110\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 972.61\n",
      "Num timesteps: 680000; Episodes: 4112\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 967.84\n",
      "Saving new model file...\n",
      "Num timesteps: 681000; Episodes: 4115\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 963.90\n",
      "Num timesteps: 682000; Episodes: 4118\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 959.93\n",
      "Num timesteps: 683000; Episodes: 4121\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 963.74\n",
      "Num timesteps: 684000; Episodes: 4123\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 971.12\n",
      "Num timesteps: 685000; Episodes: 4126\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 986.02\n",
      "Num timesteps: 686000; Episodes: 4129\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 982.72\n",
      "Num timesteps: 687000; Episodes: 4131\n",
      "Best mean reward: 992.72 - Last mean reward per episode: 994.98\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 688000; Episodes: 4134\n",
      "Best mean reward: 994.98 - Last mean reward per episode: 1014.72\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 689000; Episodes: 4136\n",
      "Best mean reward: 1014.72 - Last mean reward per episode: 1025.82\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 690000; Episodes: 4141\n",
      "Best mean reward: 1025.82 - Last mean reward per episode: 1005.44\n",
      "Saving new model file...\n",
      "Num timesteps: 691000; Episodes: 4143\n",
      "Best mean reward: 1025.82 - Last mean reward per episode: 1017.95\n",
      "Num timesteps: 692000; Episodes: 4148\n",
      "Best mean reward: 1025.82 - Last mean reward per episode: 1018.79\n",
      "Num timesteps: 693000; Episodes: 4150\n",
      "Best mean reward: 1025.82 - Last mean reward per episode: 1032.49\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 694000; Episodes: 4153\n",
      "Best mean reward: 1032.49 - Last mean reward per episode: 1026.50\n",
      "Num timesteps: 695000; Episodes: 4155\n",
      "Best mean reward: 1032.49 - Last mean reward per episode: 1033.90\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 696000; Episodes: 4159\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1019.53\n",
      "Num timesteps: 697000; Episodes: 4162\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1005.15\n",
      "Num timesteps: 698000; Episodes: 4166\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1003.62\n",
      "Num timesteps: 699000; Episodes: 4168\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1022.89\n",
      "Num timesteps: 700000; Episodes: 4170\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1020.56\n",
      "Saving new model file...\n",
      "Num timesteps: 701000; Episodes: 4172\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1033.36\n",
      "Num timesteps: 702000; Episodes: 4176\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1018.99\n",
      "Num timesteps: 703000; Episodes: 4180\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1003.97\n",
      "Num timesteps: 704000; Episodes: 4183\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 996.70\n",
      "Num timesteps: 705000; Episodes: 4185\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1003.02\n",
      "Num timesteps: 706000; Episodes: 4188\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 981.07\n",
      "Num timesteps: 707000; Episodes: 4190\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 977.56\n",
      "Num timesteps: 708000; Episodes: 4193\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 976.77\n",
      "Num timesteps: 709000; Episodes: 4196\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 973.22\n",
      "Num timesteps: 710000; Episodes: 4198\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 974.94\n",
      "Saving new model file...\n",
      "Num timesteps: 711000; Episodes: 4200\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 986.53\n",
      "Num timesteps: 712000; Episodes: 4204\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 993.16\n",
      "Num timesteps: 713000; Episodes: 4207\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 994.06\n",
      "Num timesteps: 714000; Episodes: 4210\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 989.23\n",
      "Num timesteps: 715000; Episodes: 4213\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 980.67\n",
      "Num timesteps: 716000; Episodes: 4216\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 984.82\n",
      "Num timesteps: 717000; Episodes: 4218\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 995.30\n",
      "Num timesteps: 718000; Episodes: 4221\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 993.79\n",
      "Num timesteps: 719000; Episodes: 4225\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 971.01\n",
      "Num timesteps: 720000; Episodes: 4228\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 963.42\n",
      "Saving new model file...\n",
      "Num timesteps: 721000; Episodes: 4230\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 966.40\n",
      "Num timesteps: 722000; Episodes: 4233\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 950.58\n",
      "Num timesteps: 723000; Episodes: 4236\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 952.21\n",
      "Num timesteps: 724000; Episodes: 4240\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 948.45\n",
      "Num timesteps: 725000; Episodes: 4242\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 957.85\n",
      "Num timesteps: 726000; Episodes: 4245\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 958.09\n",
      "Num timesteps: 727000; Episodes: 4247\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 977.34\n",
      "Num timesteps: 728000; Episodes: 4250\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 984.35\n",
      "Num timesteps: 729000; Episodes: 4252\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 995.01\n",
      "Num timesteps: 730000; Episodes: 4255\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 976.64\n",
      "Saving new model file...\n",
      "Num timesteps: 731000; Episodes: 4259\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 984.70\n",
      "Num timesteps: 732000; Episodes: 4264\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 983.10\n",
      "Num timesteps: 733000; Episodes: 4266\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 969.88\n",
      "Num timesteps: 734000; Episodes: 4269\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 962.90\n",
      "Num timesteps: 735000; Episodes: 4272\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 955.58\n",
      "Num timesteps: 736000; Episodes: 4275\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 955.32\n",
      "Num timesteps: 737000; Episodes: 4277\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 965.21\n",
      "Num timesteps: 738000; Episodes: 4280\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 975.51\n",
      "Num timesteps: 739000; Episodes: 4284\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 971.73\n",
      "Num timesteps: 740000; Episodes: 4286\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 978.58\n",
      "Saving new model file...\n",
      "Num timesteps: 741000; Episodes: 4288\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 986.05\n",
      "Num timesteps: 742000; Episodes: 4291\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 982.83\n",
      "Num timesteps: 743000; Episodes: 4294\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 995.85\n",
      "Num timesteps: 744000; Episodes: 4297\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 982.81\n",
      "Num timesteps: 745000; Episodes: 4299\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 983.17\n",
      "Num timesteps: 746000; Episodes: 4301\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 984.73\n",
      "Num timesteps: 747000; Episodes: 4305\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 991.27\n",
      "Num timesteps: 748000; Episodes: 4308\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 998.66\n",
      "Num timesteps: 749000; Episodes: 4310\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1002.72\n",
      "Num timesteps: 750000; Episodes: 4313\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1016.72\n",
      "Saving new model file...\n",
      "Num timesteps: 751000; Episodes: 4315\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1021.29\n",
      "Num timesteps: 752000; Episodes: 4320\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 984.50\n",
      "Num timesteps: 753000; Episodes: 4323\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 991.12\n",
      "Num timesteps: 754000; Episodes: 4327\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 982.73\n",
      "Num timesteps: 755000; Episodes: 4329\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1000.03\n",
      "Num timesteps: 756000; Episodes: 4331\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 996.93\n",
      "Num timesteps: 757000; Episodes: 4335\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 992.71\n",
      "Num timesteps: 758000; Episodes: 4338\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 994.62\n",
      "Num timesteps: 759000; Episodes: 4340\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1010.39\n",
      "Num timesteps: 760000; Episodes: 4342\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1005.63\n",
      "Saving new model file...\n",
      "Num timesteps: 761000; Episodes: 4345\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1009.62\n",
      "Num timesteps: 762000; Episodes: 4348\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 996.51\n",
      "Num timesteps: 763000; Episodes: 4351\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 989.23\n",
      "Num timesteps: 764000; Episodes: 4354\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 992.49\n",
      "Num timesteps: 765000; Episodes: 4356\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1000.28\n",
      "Num timesteps: 766000; Episodes: 4358\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1006.13\n",
      "Num timesteps: 767000; Episodes: 4360\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1030.69\n",
      "Num timesteps: 768000; Episodes: 4363\n",
      "Best mean reward: 1033.90 - Last mean reward per episode: 1044.08\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 769000; Episodes: 4366\n",
      "Best mean reward: 1044.08 - Last mean reward per episode: 1049.82\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 770000; Episodes: 4369\n",
      "Best mean reward: 1049.82 - Last mean reward per episode: 1054.28\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 771000; Episodes: 4372\n",
      "Best mean reward: 1054.28 - Last mean reward per episode: 1049.32\n",
      "Num timesteps: 772000; Episodes: 4374\n",
      "Best mean reward: 1054.28 - Last mean reward per episode: 1063.25\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 773000; Episodes: 4377\n",
      "Best mean reward: 1063.25 - Last mean reward per episode: 1063.41\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 774000; Episodes: 4379\n",
      "Best mean reward: 1063.41 - Last mean reward per episode: 1063.05\n",
      "Num timesteps: 775000; Episodes: 4382\n",
      "Best mean reward: 1063.41 - Last mean reward per episode: 1060.13\n",
      "Num timesteps: 776000; Episodes: 4386\n",
      "Best mean reward: 1063.41 - Last mean reward per episode: 1054.12\n",
      "Num timesteps: 777000; Episodes: 4388\n",
      "Best mean reward: 1063.41 - Last mean reward per episode: 1061.74\n",
      "Num timesteps: 778000; Episodes: 4390\n",
      "Best mean reward: 1063.41 - Last mean reward per episode: 1061.82\n",
      "Num timesteps: 779000; Episodes: 4392\n",
      "Best mean reward: 1063.41 - Last mean reward per episode: 1071.82\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 780000; Episodes: 4395\n",
      "Best mean reward: 1071.82 - Last mean reward per episode: 1081.49\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 781000; Episodes: 4399\n",
      "Best mean reward: 1081.49 - Last mean reward per episode: 1062.15\n",
      "Num timesteps: 782000; Episodes: 4401\n",
      "Best mean reward: 1081.49 - Last mean reward per episode: 1067.10\n",
      "Num timesteps: 783000; Episodes: 4403\n",
      "Best mean reward: 1081.49 - Last mean reward per episode: 1069.09\n",
      "Num timesteps: 784000; Episodes: 4406\n",
      "Best mean reward: 1081.49 - Last mean reward per episode: 1083.27\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 785000; Episodes: 4409\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1071.46\n",
      "Num timesteps: 786000; Episodes: 4412\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1054.01\n",
      "Num timesteps: 787000; Episodes: 4415\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1037.46\n",
      "Num timesteps: 788000; Episodes: 4417\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1049.24\n",
      "Num timesteps: 789000; Episodes: 4420\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1077.29\n",
      "Num timesteps: 790000; Episodes: 4423\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1072.17\n",
      "Saving new model file...\n",
      "Num timesteps: 791000; Episodes: 4427\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1077.55\n",
      "Num timesteps: 792000; Episodes: 4432\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1046.97\n",
      "Num timesteps: 793000; Episodes: 4435\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1057.15\n",
      "Num timesteps: 794000; Episodes: 4438\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1053.80\n",
      "Num timesteps: 795000; Episodes: 4441\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1056.77\n",
      "Num timesteps: 796000; Episodes: 4443\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1049.94\n",
      "Num timesteps: 797000; Episodes: 4446\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1054.68\n",
      "Num timesteps: 798000; Episodes: 4450\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1040.26\n",
      "Num timesteps: 799000; Episodes: 4452\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1050.50\n",
      "Num timesteps: 800000; Episodes: 4455\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1054.58\n",
      "Saving new model file...\n",
      "Num timesteps: 801000; Episodes: 4458\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1033.77\n",
      "Num timesteps: 802000; Episodes: 4460\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1033.20\n",
      "Num timesteps: 803000; Episodes: 4464\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1031.06\n",
      "Num timesteps: 804000; Episodes: 4466\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1038.49\n",
      "Num timesteps: 805000; Episodes: 4468\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1037.94\n",
      "Num timesteps: 806000; Episodes: 4470\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1051.30\n",
      "Num timesteps: 807000; Episodes: 4472\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1060.94\n",
      "Num timesteps: 808000; Episodes: 4475\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1049.53\n",
      "Num timesteps: 809000; Episodes: 4478\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1038.76\n",
      "Num timesteps: 810000; Episodes: 4481\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1052.13\n",
      "Saving new model file...\n",
      "Num timesteps: 811000; Episodes: 4484\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1044.78\n",
      "Num timesteps: 812000; Episodes: 4487\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1043.16\n",
      "Num timesteps: 813000; Episodes: 4490\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1033.56\n",
      "Num timesteps: 814000; Episodes: 4492\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1034.88\n",
      "Num timesteps: 815000; Episodes: 4495\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1019.63\n",
      "Num timesteps: 816000; Episodes: 4498\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1038.80\n",
      "Num timesteps: 817000; Episodes: 4501\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1024.65\n",
      "Num timesteps: 818000; Episodes: 4504\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1009.67\n",
      "Num timesteps: 819000; Episodes: 4507\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1027.63\n",
      "Num timesteps: 820000; Episodes: 4510\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1021.70\n",
      "Saving new model file...\n",
      "Num timesteps: 821000; Episodes: 4512\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1035.81\n",
      "Num timesteps: 822000; Episodes: 4516\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1022.87\n",
      "Num timesteps: 823000; Episodes: 4519\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1015.95\n",
      "Num timesteps: 824000; Episodes: 4522\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1020.93\n",
      "Num timesteps: 825000; Episodes: 4525\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1005.91\n",
      "Num timesteps: 826000; Episodes: 4528\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1021.80\n",
      "Num timesteps: 827000; Episodes: 4530\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1031.71\n",
      "Num timesteps: 828000; Episodes: 4533\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1050.13\n",
      "Num timesteps: 829000; Episodes: 4535\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1054.23\n",
      "Num timesteps: 830000; Episodes: 4538\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1059.98\n",
      "Saving new model file...\n",
      "Num timesteps: 831000; Episodes: 4541\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1066.19\n",
      "Num timesteps: 832000; Episodes: 4543\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1064.77\n",
      "Num timesteps: 833000; Episodes: 4547\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1057.44\n",
      "Num timesteps: 834000; Episodes: 4551\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1056.03\n",
      "Num timesteps: 835000; Episodes: 4554\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1041.68\n",
      "Num timesteps: 836000; Episodes: 4559\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1028.32\n",
      "Num timesteps: 837000; Episodes: 4562\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1024.89\n",
      "Num timesteps: 838000; Episodes: 4565\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1028.95\n",
      "Num timesteps: 839000; Episodes: 4567\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1020.48\n",
      "Num timesteps: 840000; Episodes: 4569\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1021.05\n",
      "Saving new model file...\n",
      "Num timesteps: 841000; Episodes: 4572\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1005.30\n",
      "Num timesteps: 842000; Episodes: 4576\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 998.26\n",
      "Num timesteps: 843000; Episodes: 4578\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1009.16\n",
      "Num timesteps: 844000; Episodes: 4580\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1005.78\n",
      "Num timesteps: 845000; Episodes: 4582\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1021.58\n",
      "Num timesteps: 846000; Episodes: 4584\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1027.80\n",
      "Num timesteps: 847000; Episodes: 4587\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1029.03\n",
      "Num timesteps: 848000; Episodes: 4589\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1028.72\n",
      "Num timesteps: 849000; Episodes: 4592\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1031.48\n",
      "Num timesteps: 850000; Episodes: 4595\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1033.93\n",
      "Saving new model file...\n",
      "Num timesteps: 851000; Episodes: 4599\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1013.57\n",
      "Num timesteps: 852000; Episodes: 4601\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1027.74\n",
      "Num timesteps: 853000; Episodes: 4604\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1040.61\n",
      "Num timesteps: 854000; Episodes: 4606\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1040.32\n",
      "Num timesteps: 855000; Episodes: 4610\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1037.11\n",
      "Num timesteps: 856000; Episodes: 4612\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1037.53\n",
      "Num timesteps: 857000; Episodes: 4615\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1056.14\n",
      "Num timesteps: 858000; Episodes: 4618\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1054.30\n",
      "Num timesteps: 859000; Episodes: 4620\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1044.92\n",
      "Num timesteps: 860000; Episodes: 4624\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1052.56\n",
      "Saving new model file...\n",
      "Num timesteps: 861000; Episodes: 4628\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1038.18\n",
      "Num timesteps: 862000; Episodes: 4631\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1027.83\n",
      "Num timesteps: 863000; Episodes: 4634\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1021.78\n",
      "Num timesteps: 864000; Episodes: 4637\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1022.49\n",
      "Num timesteps: 865000; Episodes: 4640\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1019.03\n",
      "Num timesteps: 866000; Episodes: 4642\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1025.07\n",
      "Num timesteps: 867000; Episodes: 4645\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1023.04\n",
      "Num timesteps: 868000; Episodes: 4648\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1022.74\n",
      "Num timesteps: 869000; Episodes: 4650\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1030.53\n",
      "Num timesteps: 870000; Episodes: 4653\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1061.62\n",
      "Saving new model file...\n",
      "Num timesteps: 871000; Episodes: 4655\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1059.44\n",
      "Num timesteps: 872000; Episodes: 4657\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1070.32\n",
      "Num timesteps: 873000; Episodes: 4660\n",
      "Best mean reward: 1083.27 - Last mean reward per episode: 1094.19\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 874000; Episodes: 4662\n",
      "Best mean reward: 1094.19 - Last mean reward per episode: 1093.63\n",
      "Num timesteps: 875000; Episodes: 4664\n",
      "Best mean reward: 1094.19 - Last mean reward per episode: 1097.22\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 876000; Episodes: 4667\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1090.57\n",
      "Num timesteps: 877000; Episodes: 4670\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1084.56\n",
      "Num timesteps: 878000; Episodes: 4673\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1087.01\n",
      "Num timesteps: 879000; Episodes: 4678\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1060.27\n",
      "Num timesteps: 880000; Episodes: 4680\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1065.41\n",
      "Saving new model file...\n",
      "Num timesteps: 881000; Episodes: 4683\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1050.24\n",
      "Num timesteps: 882000; Episodes: 4687\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1032.27\n",
      "Num timesteps: 883000; Episodes: 4689\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1026.09\n",
      "Num timesteps: 884000; Episodes: 4692\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1020.88\n",
      "Num timesteps: 885000; Episodes: 4695\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1021.03\n",
      "Num timesteps: 886000; Episodes: 4697\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1037.66\n",
      "Num timesteps: 887000; Episodes: 4699\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1041.17\n",
      "Num timesteps: 888000; Episodes: 4703\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1025.40\n",
      "Num timesteps: 889000; Episodes: 4705\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1032.92\n",
      "Num timesteps: 890000; Episodes: 4707\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1033.50\n",
      "Saving new model file...\n",
      "Num timesteps: 891000; Episodes: 4710\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1052.95\n",
      "Num timesteps: 892000; Episodes: 4714\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1035.12\n",
      "Num timesteps: 893000; Episodes: 4716\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1034.18\n",
      "Num timesteps: 894000; Episodes: 4719\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1036.28\n",
      "Num timesteps: 895000; Episodes: 4722\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1043.27\n",
      "Num timesteps: 896000; Episodes: 4724\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1049.15\n",
      "Num timesteps: 897000; Episodes: 4726\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1055.69\n",
      "Num timesteps: 898000; Episodes: 4728\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1082.28\n",
      "Num timesteps: 899000; Episodes: 4731\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1090.72\n",
      "Num timesteps: 900000; Episodes: 4735\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1070.44\n",
      "Saving new model file...\n",
      "Num timesteps: 901000; Episodes: 4738\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1073.36\n",
      "Num timesteps: 902000; Episodes: 4740\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1086.43\n",
      "Num timesteps: 903000; Episodes: 4742\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1086.41\n",
      "Num timesteps: 904000; Episodes: 4745\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1096.58\n",
      "Num timesteps: 905000; Episodes: 4747\n",
      "Best mean reward: 1097.22 - Last mean reward per episode: 1102.95\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 906000; Episodes: 4749\n",
      "Best mean reward: 1102.95 - Last mean reward per episode: 1119.84\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 907000; Episodes: 4751\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1114.70\n",
      "Num timesteps: 908000; Episodes: 4754\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1114.93\n",
      "Num timesteps: 909000; Episodes: 4757\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1103.77\n",
      "Num timesteps: 910000; Episodes: 4759\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1109.01\n",
      "Saving new model file...\n",
      "Num timesteps: 911000; Episodes: 4762\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1100.94\n",
      "Num timesteps: 912000; Episodes: 4767\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1082.75\n",
      "Num timesteps: 913000; Episodes: 4769\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1077.17\n",
      "Num timesteps: 914000; Episodes: 4771\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1079.28\n",
      "Num timesteps: 915000; Episodes: 4774\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1100.01\n",
      "Num timesteps: 916000; Episodes: 4776\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1100.67\n",
      "Num timesteps: 917000; Episodes: 4779\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1114.02\n",
      "Num timesteps: 918000; Episodes: 4782\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1104.66\n",
      "Num timesteps: 919000; Episodes: 4785\n",
      "Best mean reward: 1119.84 - Last mean reward per episode: 1124.59\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 920000; Episodes: 4787\n",
      "Best mean reward: 1124.59 - Last mean reward per episode: 1126.12\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving new model file...\n",
      "Num timesteps: 921000; Episodes: 4790\n",
      "Best mean reward: 1126.12 - Last mean reward per episode: 1137.98\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 922000; Episodes: 4792\n",
      "Best mean reward: 1137.98 - Last mean reward per episode: 1138.55\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 923000; Episodes: 4795\n",
      "Best mean reward: 1138.55 - Last mean reward per episode: 1139.99\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 924000; Episodes: 4797\n",
      "Best mean reward: 1139.99 - Last mean reward per episode: 1149.29\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 925000; Episodes: 4800\n",
      "Best mean reward: 1149.29 - Last mean reward per episode: 1143.90\n",
      "Num timesteps: 926000; Episodes: 4803\n",
      "Best mean reward: 1149.29 - Last mean reward per episode: 1151.85\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 927000; Episodes: 4805\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1148.87\n",
      "Num timesteps: 928000; Episodes: 4810\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1111.94\n",
      "Num timesteps: 929000; Episodes: 4812\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1119.66\n",
      "Num timesteps: 930000; Episodes: 4814\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1138.17\n",
      "Saving new model file...\n",
      "Num timesteps: 931000; Episodes: 4817\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1135.67\n",
      "Num timesteps: 932000; Episodes: 4819\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1143.78\n",
      "Num timesteps: 933000; Episodes: 4822\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1144.74\n",
      "Num timesteps: 934000; Episodes: 4825\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1121.90\n",
      "Num timesteps: 935000; Episodes: 4827\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1119.98\n",
      "Num timesteps: 936000; Episodes: 4829\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1120.24\n",
      "Num timesteps: 937000; Episodes: 4833\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1117.44\n",
      "Num timesteps: 938000; Episodes: 4836\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1126.57\n",
      "Num timesteps: 939000; Episodes: 4838\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1131.71\n",
      "Num timesteps: 940000; Episodes: 4841\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1130.69\n",
      "Saving new model file...\n",
      "Num timesteps: 941000; Episodes: 4844\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1116.36\n",
      "Num timesteps: 942000; Episodes: 4846\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1115.00\n",
      "Num timesteps: 943000; Episodes: 4848\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1116.86\n",
      "Num timesteps: 944000; Episodes: 4851\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1113.85\n",
      "Num timesteps: 945000; Episodes: 4854\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1100.76\n",
      "Num timesteps: 946000; Episodes: 4857\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1100.25\n",
      "Num timesteps: 947000; Episodes: 4861\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1092.15\n",
      "Num timesteps: 948000; Episodes: 4864\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1091.97\n",
      "Num timesteps: 949000; Episodes: 4867\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1107.13\n",
      "Num timesteps: 950000; Episodes: 4869\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1118.82\n",
      "Saving new model file...\n",
      "Num timesteps: 951000; Episodes: 4871\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1121.05\n",
      "Num timesteps: 952000; Episodes: 4874\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1111.86\n",
      "Num timesteps: 953000; Episodes: 4877\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1123.51\n",
      "Num timesteps: 954000; Episodes: 4881\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1110.93\n",
      "Num timesteps: 955000; Episodes: 4884\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1098.90\n",
      "Num timesteps: 956000; Episodes: 4886\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1110.31\n",
      "Num timesteps: 957000; Episodes: 4889\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1109.14\n",
      "Num timesteps: 958000; Episodes: 4891\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1110.27\n",
      "Num timesteps: 959000; Episodes: 4894\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1117.63\n",
      "Num timesteps: 960000; Episodes: 4896\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1105.72\n",
      "Saving new model file...\n",
      "Num timesteps: 961000; Episodes: 4899\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1107.49\n",
      "Num timesteps: 962000; Episodes: 4902\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1098.51\n",
      "Num timesteps: 963000; Episodes: 4905\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1091.59\n",
      "Num timesteps: 964000; Episodes: 4908\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1097.27\n",
      "Num timesteps: 965000; Episodes: 4910\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1116.21\n",
      "Num timesteps: 966000; Episodes: 4913\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1105.55\n",
      "Num timesteps: 967000; Episodes: 4916\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1086.82\n",
      "Num timesteps: 968000; Episodes: 4920\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1068.81\n",
      "Num timesteps: 969000; Episodes: 4923\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1081.48\n",
      "Num timesteps: 970000; Episodes: 4927\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1072.60\n",
      "Saving new model file...\n",
      "Num timesteps: 971000; Episodes: 4930\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1061.42\n",
      "Num timesteps: 972000; Episodes: 4932\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1073.74\n",
      "Num timesteps: 973000; Episodes: 4935\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1063.65\n",
      "Num timesteps: 974000; Episodes: 4939\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1058.50\n",
      "Num timesteps: 975000; Episodes: 4942\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1047.80\n",
      "Num timesteps: 976000; Episodes: 4944\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1057.06\n",
      "Num timesteps: 977000; Episodes: 4946\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1058.63\n",
      "Num timesteps: 978000; Episodes: 4950\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1039.19\n",
      "Num timesteps: 979000; Episodes: 4953\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1043.48\n",
      "Num timesteps: 980000; Episodes: 4955\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1050.02\n",
      "Saving new model file...\n",
      "Num timesteps: 981000; Episodes: 4958\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1059.40\n",
      "Num timesteps: 982000; Episodes: 4961\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1061.45\n",
      "Num timesteps: 983000; Episodes: 4964\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1065.96\n",
      "Num timesteps: 984000; Episodes: 4966\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1077.45\n",
      "Num timesteps: 985000; Episodes: 4969\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1070.59\n",
      "Num timesteps: 986000; Episodes: 4972\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1060.67\n",
      "Num timesteps: 987000; Episodes: 4975\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1062.34\n",
      "Num timesteps: 988000; Episodes: 4979\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1057.68\n",
      "Num timesteps: 989000; Episodes: 4983\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1043.91\n",
      "Num timesteps: 990000; Episodes: 4986\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1054.05\n",
      "Saving new model file...\n",
      "Num timesteps: 991000; Episodes: 4988\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1048.20\n",
      "Num timesteps: 992000; Episodes: 4991\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1043.75\n",
      "Num timesteps: 993000; Episodes: 4993\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1044.04\n",
      "Num timesteps: 994000; Episodes: 4996\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1051.58\n",
      "Num timesteps: 995000; Episodes: 4998\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1040.65\n",
      "Num timesteps: 996000; Episodes: 5001\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1053.88\n",
      "Num timesteps: 997000; Episodes: 5003\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1058.22\n",
      "Num timesteps: 998000; Episodes: 5005\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1068.43\n",
      "Num timesteps: 999000; Episodes: 5007\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1067.67\n",
      "Num timesteps: 1000000; Episodes: 5010\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1081.79\n",
      "Saving new model file...\n",
      "Num timesteps: 1001000; Episodes: 5013\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1079.16\n",
      "Num timesteps: 1002000; Episodes: 5016\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1089.36\n",
      "Num timesteps: 1003000; Episodes: 5018\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1102.65\n",
      "Num timesteps: 1004000; Episodes: 5021\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1108.32\n",
      "Num timesteps: 1005000; Episodes: 5024\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1104.65\n",
      "Num timesteps: 1006000; Episodes: 5026\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1116.77\n",
      "Num timesteps: 1007000; Episodes: 5029\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1120.72\n",
      "Num timesteps: 1008000; Episodes: 5032\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1106.15\n",
      "Num timesteps: 1009000; Episodes: 5035\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1108.72\n",
      "Num timesteps: 1010000; Episodes: 5037\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1128.74\n",
      "Saving new model file...\n",
      "Num timesteps: 1011000; Episodes: 5040\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1119.88\n",
      "Num timesteps: 1012000; Episodes: 5044\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1115.46\n",
      "Num timesteps: 1013000; Episodes: 5047\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1104.47\n",
      "Num timesteps: 1014000; Episodes: 5049\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1119.59\n",
      "Num timesteps: 1015000; Episodes: 5052\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1119.85\n",
      "Num timesteps: 1016000; Episodes: 5056\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1093.41\n",
      "Num timesteps: 1017000; Episodes: 5059\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1078.16\n",
      "Num timesteps: 1018000; Episodes: 5062\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1089.81\n",
      "Num timesteps: 1019000; Episodes: 5066\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1065.11\n",
      "Num timesteps: 1020000; Episodes: 5069\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1058.55\n",
      "Saving new model file...\n",
      "Num timesteps: 1021000; Episodes: 5073\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1049.88\n",
      "Num timesteps: 1022000; Episodes: 5076\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1033.04\n",
      "Num timesteps: 1023000; Episodes: 5079\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1039.28\n",
      "Num timesteps: 1024000; Episodes: 5082\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1055.09\n",
      "Num timesteps: 1025000; Episodes: 5084\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1063.22\n",
      "Num timesteps: 1026000; Episodes: 5087\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1056.01\n",
      "Num timesteps: 1027000; Episodes: 5089\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1057.22\n",
      "Num timesteps: 1028000; Episodes: 5092\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1051.32\n",
      "Num timesteps: 1029000; Episodes: 5096\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1040.27\n",
      "Num timesteps: 1030000; Episodes: 5100\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1027.81\n",
      "Saving new model file...\n",
      "Num timesteps: 1031000; Episodes: 5102\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1028.97\n",
      "Num timesteps: 1032000; Episodes: 5107\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1002.84\n",
      "Num timesteps: 1033000; Episodes: 5110\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 994.03\n",
      "Num timesteps: 1034000; Episodes: 5112\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 998.20\n",
      "Num timesteps: 1035000; Episodes: 5117\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 978.69\n",
      "Num timesteps: 1036000; Episodes: 5121\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 956.34\n",
      "Num timesteps: 1037000; Episodes: 5123\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 965.63\n",
      "Num timesteps: 1038000; Episodes: 5125\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 966.46\n",
      "Num timesteps: 1039000; Episodes: 5128\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 962.36\n",
      "Num timesteps: 1040000; Episodes: 5130\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 971.32\n",
      "Saving new model file...\n",
      "Num timesteps: 1041000; Episodes: 5132\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 984.97\n",
      "Num timesteps: 1042000; Episodes: 5135\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 993.10\n",
      "Num timesteps: 1043000; Episodes: 5137\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 991.86\n",
      "Num timesteps: 1044000; Episodes: 5140\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 995.62\n",
      "Num timesteps: 1045000; Episodes: 5143\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 986.24\n",
      "Num timesteps: 1046000; Episodes: 5146\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1000.88\n",
      "Num timesteps: 1047000; Episodes: 5148\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 999.09\n",
      "Num timesteps: 1048000; Episodes: 5152\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 986.39\n",
      "Num timesteps: 1049000; Episodes: 5156\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 995.81\n",
      "Num timesteps: 1050000; Episodes: 5159\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 998.89\n",
      "Saving new model file...\n",
      "Num timesteps: 1051000; Episodes: 5162\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 990.63\n",
      "Num timesteps: 1052000; Episodes: 5164\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1009.22\n",
      "Num timesteps: 1053000; Episodes: 5168\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 986.10\n",
      "Num timesteps: 1054000; Episodes: 5171\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1012.12\n",
      "Num timesteps: 1055000; Episodes: 5173\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1024.11\n",
      "Num timesteps: 1056000; Episodes: 5175\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1030.79\n",
      "Num timesteps: 1057000; Episodes: 5177\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1032.12\n",
      "Num timesteps: 1058000; Episodes: 5180\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1053.42\n",
      "Num timesteps: 1059000; Episodes: 5183\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1042.02\n",
      "Num timesteps: 1060000; Episodes: 5188\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1017.77\n",
      "Saving new model file...\n",
      "Num timesteps: 1061000; Episodes: 5190\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1010.38\n",
      "Num timesteps: 1062000; Episodes: 5193\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1022.73\n",
      "Num timesteps: 1063000; Episodes: 5195\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1045.90\n",
      "Num timesteps: 1064000; Episodes: 5198\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1028.99\n",
      "Num timesteps: 1065000; Episodes: 5203\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1022.67\n",
      "Num timesteps: 1066000; Episodes: 5207\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1028.49\n",
      "Num timesteps: 1067000; Episodes: 5209\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1028.32\n",
      "Num timesteps: 1068000; Episodes: 5212\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1033.44\n",
      "Num timesteps: 1069000; Episodes: 5214\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1042.48\n",
      "Num timesteps: 1070000; Episodes: 5216\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1056.31\n",
      "Saving new model file...\n",
      "Num timesteps: 1071000; Episodes: 5219\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1067.67\n",
      "Num timesteps: 1072000; Episodes: 5222\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1074.38\n",
      "Num timesteps: 1073000; Episodes: 5226\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1041.94\n",
      "Num timesteps: 1074000; Episodes: 5229\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1032.88\n",
      "Num timesteps: 1075000; Episodes: 5232\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1029.15\n",
      "Num timesteps: 1076000; Episodes: 5235\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1025.79\n",
      "Num timesteps: 1077000; Episodes: 5238\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1016.10\n",
      "Num timesteps: 1078000; Episodes: 5240\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1023.48\n",
      "Num timesteps: 1079000; Episodes: 5243\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1041.75\n",
      "Num timesteps: 1080000; Episodes: 5245\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1043.13\n",
      "Saving new model file...\n",
      "Num timesteps: 1081000; Episodes: 5247\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1040.16\n",
      "Num timesteps: 1082000; Episodes: 5249\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1038.69\n",
      "Num timesteps: 1083000; Episodes: 5252\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1053.38\n",
      "Num timesteps: 1084000; Episodes: 5254\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1070.00\n",
      "Num timesteps: 1085000; Episodes: 5257\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1067.71\n",
      "Num timesteps: 1086000; Episodes: 5261\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1078.79\n",
      "Num timesteps: 1087000; Episodes: 5264\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1059.28\n",
      "Num timesteps: 1088000; Episodes: 5267\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1077.87\n",
      "Num timesteps: 1089000; Episodes: 5269\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1086.37\n",
      "Num timesteps: 1090000; Episodes: 5271\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1086.49\n",
      "Saving new model file...\n",
      "Num timesteps: 1091000; Episodes: 5273\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1087.29\n",
      "Num timesteps: 1092000; Episodes: 5276\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1098.05\n",
      "Num timesteps: 1093000; Episodes: 5280\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1069.29\n",
      "Num timesteps: 1094000; Episodes: 5282\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1076.85\n",
      "Num timesteps: 1095000; Episodes: 5285\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1086.59\n",
      "Num timesteps: 1096000; Episodes: 5287\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1104.52\n",
      "Num timesteps: 1097000; Episodes: 5289\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1110.62\n",
      "Num timesteps: 1098000; Episodes: 5292\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1113.45\n",
      "Num timesteps: 1099000; Episodes: 5294\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1113.41\n",
      "Num timesteps: 1100000; Episodes: 5297\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1130.45\n",
      "Saving new model file...\n",
      "Num timesteps: 1101000; Episodes: 5299\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1121.25\n",
      "Num timesteps: 1102000; Episodes: 5302\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1144.48\n",
      "Num timesteps: 1103000; Episodes: 5305\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1145.29\n",
      "Num timesteps: 1104000; Episodes: 5308\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1149.07\n",
      "Num timesteps: 1105000; Episodes: 5309\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1148.06\n",
      "Num timesteps: 1106000; Episodes: 5312\n",
      "Best mean reward: 1151.85 - Last mean reward per episode: 1157.19\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1107000; Episodes: 5316\n",
      "Best mean reward: 1157.19 - Last mean reward per episode: 1133.81\n",
      "Num timesteps: 1108000; Episodes: 5318\n",
      "Best mean reward: 1157.19 - Last mean reward per episode: 1143.63\n",
      "Num timesteps: 1109000; Episodes: 5320\n",
      "Best mean reward: 1157.19 - Last mean reward per episode: 1143.96\n",
      "Num timesteps: 1110000; Episodes: 5323\n",
      "Best mean reward: 1157.19 - Last mean reward per episode: 1147.90\n",
      "Saving new model file...\n",
      "Num timesteps: 1111000; Episodes: 5325\n",
      "Best mean reward: 1157.19 - Last mean reward per episode: 1163.81\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1112000; Episodes: 5327\n",
      "Best mean reward: 1163.81 - Last mean reward per episode: 1187.04\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1113000; Episodes: 5330\n",
      "Best mean reward: 1187.04 - Last mean reward per episode: 1195.61\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1114000; Episodes: 5332\n",
      "Best mean reward: 1195.61 - Last mean reward per episode: 1192.94\n",
      "Num timesteps: 1115000; Episodes: 5334\n",
      "Best mean reward: 1195.61 - Last mean reward per episode: 1195.28\n",
      "Num timesteps: 1116000; Episodes: 5337\n",
      "Best mean reward: 1195.61 - Last mean reward per episode: 1202.86\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1117000; Episodes: 5340\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1188.17\n",
      "Num timesteps: 1118000; Episodes: 5342\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1187.80\n",
      "Num timesteps: 1119000; Episodes: 5345\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1192.08\n",
      "Num timesteps: 1120000; Episodes: 5347\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1198.86\n",
      "Saving new model file...\n",
      "Num timesteps: 1121000; Episodes: 5351\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1178.95\n",
      "Num timesteps: 1122000; Episodes: 5353\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1177.66\n",
      "Num timesteps: 1123000; Episodes: 5356\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1171.27\n",
      "Num timesteps: 1124000; Episodes: 5358\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1175.10\n",
      "Num timesteps: 1125000; Episodes: 5361\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1183.21\n",
      "Num timesteps: 1126000; Episodes: 5363\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1196.74\n",
      "Num timesteps: 1127000; Episodes: 5367\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1183.53\n",
      "Num timesteps: 1128000; Episodes: 5370\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1170.52\n",
      "Num timesteps: 1129000; Episodes: 5372\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1165.93\n",
      "Num timesteps: 1130000; Episodes: 5374\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1165.98\n",
      "Saving new model file...\n",
      "Num timesteps: 1131000; Episodes: 5377\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1158.10\n",
      "Num timesteps: 1132000; Episodes: 5379\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1181.80\n",
      "Num timesteps: 1133000; Episodes: 5381\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1187.14\n",
      "Num timesteps: 1134000; Episodes: 5384\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1196.64\n",
      "Num timesteps: 1135000; Episodes: 5387\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1186.31\n",
      "Num timesteps: 1136000; Episodes: 5389\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1186.71\n",
      "Num timesteps: 1137000; Episodes: 5392\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1185.87\n",
      "Num timesteps: 1138000; Episodes: 5394\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1185.59\n",
      "Num timesteps: 1139000; Episodes: 5396\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1184.85\n",
      "Num timesteps: 1140000; Episodes: 5399\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1181.75\n",
      "Saving new model file...\n",
      "Num timesteps: 1141000; Episodes: 5402\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1169.59\n",
      "Num timesteps: 1142000; Episodes: 5404\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1178.74\n",
      "Num timesteps: 1143000; Episodes: 5407\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1171.73\n",
      "Num timesteps: 1144000; Episodes: 5410\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1167.13\n",
      "Num timesteps: 1145000; Episodes: 5412\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1166.67\n",
      "Num timesteps: 1146000; Episodes: 5415\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1178.16\n",
      "Num timesteps: 1147000; Episodes: 5418\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1171.40\n",
      "Num timesteps: 1148000; Episodes: 5420\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1170.96\n",
      "Num timesteps: 1149000; Episodes: 5424\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1157.36\n",
      "Num timesteps: 1150000; Episodes: 5427\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1146.68\n",
      "Saving new model file...\n",
      "Num timesteps: 1151000; Episodes: 5430\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1143.52\n",
      "Num timesteps: 1152000; Episodes: 5433\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1138.73\n",
      "Num timesteps: 1153000; Episodes: 5436\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1122.50\n",
      "Num timesteps: 1154000; Episodes: 5438\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1136.63\n",
      "Num timesteps: 1155000; Episodes: 5442\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1126.47\n",
      "Num timesteps: 1156000; Episodes: 5444\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1126.22\n",
      "Num timesteps: 1157000; Episodes: 5446\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1122.95\n",
      "Num timesteps: 1158000; Episodes: 5449\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1124.44\n",
      "Num timesteps: 1159000; Episodes: 5451\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1143.47\n",
      "Num timesteps: 1160000; Episodes: 5454\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1130.73\n",
      "Saving new model file...\n",
      "Num timesteps: 1161000; Episodes: 5456\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1140.26\n",
      "Num timesteps: 1162000; Episodes: 5460\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1129.03\n",
      "Num timesteps: 1163000; Episodes: 5462\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1134.26\n",
      "Num timesteps: 1164000; Episodes: 5464\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1134.67\n",
      "Num timesteps: 1165000; Episodes: 5466\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1158.18\n",
      "Num timesteps: 1166000; Episodes: 5469\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1163.46\n",
      "Num timesteps: 1167000; Episodes: 5473\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1149.88\n",
      "Num timesteps: 1168000; Episodes: 5476\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1145.66\n",
      "Num timesteps: 1169000; Episodes: 5479\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1130.53\n",
      "Num timesteps: 1170000; Episodes: 5481\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1131.70\n",
      "Saving new model file...\n",
      "Num timesteps: 1171000; Episodes: 5484\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1135.70\n",
      "Num timesteps: 1172000; Episodes: 5486\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1143.01\n",
      "Num timesteps: 1173000; Episodes: 5489\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1139.40\n",
      "Num timesteps: 1174000; Episodes: 5490\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1139.65\n",
      "Num timesteps: 1175000; Episodes: 5493\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1145.89\n",
      "Num timesteps: 1176000; Episodes: 5495\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1146.08\n",
      "Num timesteps: 1177000; Episodes: 5497\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1146.67\n",
      "Num timesteps: 1178000; Episodes: 5499\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1156.96\n",
      "Num timesteps: 1179000; Episodes: 5503\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1148.74\n",
      "Num timesteps: 1180000; Episodes: 5507\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1147.18\n",
      "Saving new model file...\n",
      "Num timesteps: 1181000; Episodes: 5509\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1156.58\n",
      "Num timesteps: 1182000; Episodes: 5512\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1153.38\n",
      "Num timesteps: 1183000; Episodes: 5514\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1153.63\n",
      "Num timesteps: 1184000; Episodes: 5517\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1153.64\n",
      "Num timesteps: 1185000; Episodes: 5521\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1134.66\n",
      "Num timesteps: 1186000; Episodes: 5523\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1147.73\n",
      "Num timesteps: 1187000; Episodes: 5525\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1161.83\n",
      "Num timesteps: 1188000; Episodes: 5528\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1174.78\n",
      "Num timesteps: 1189000; Episodes: 5530\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1176.01\n",
      "Num timesteps: 1190000; Episodes: 5533\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1183.51\n",
      "Saving new model file...\n",
      "Num timesteps: 1191000; Episodes: 5536\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1186.96\n",
      "Num timesteps: 1192000; Episodes: 5538\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1187.60\n",
      "Num timesteps: 1193000; Episodes: 5540\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1201.01\n",
      "Num timesteps: 1194000; Episodes: 5542\n",
      "Best mean reward: 1202.86 - Last mean reward per episode: 1212.86\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1195000; Episodes: 5545\n",
      "Best mean reward: 1212.86 - Last mean reward per episode: 1203.66\n",
      "Num timesteps: 1196000; Episodes: 5547\n",
      "Best mean reward: 1212.86 - Last mean reward per episode: 1205.81\n",
      "Num timesteps: 1197000; Episodes: 5550\n",
      "Best mean reward: 1212.86 - Last mean reward per episode: 1215.32\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1198000; Episodes: 5552\n",
      "Best mean reward: 1215.32 - Last mean reward per episode: 1217.04\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1199000; Episodes: 5556\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1204.54\n",
      "Num timesteps: 1200000; Episodes: 5558\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1206.54\n",
      "Saving new model file...\n",
      "Num timesteps: 1201000; Episodes: 5561\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1210.60\n",
      "Num timesteps: 1202000; Episodes: 5563\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1211.26\n",
      "Num timesteps: 1203000; Episodes: 5565\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1211.72\n",
      "Num timesteps: 1204000; Episodes: 5568\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1195.67\n",
      "Num timesteps: 1205000; Episodes: 5570\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1204.13\n",
      "Num timesteps: 1206000; Episodes: 5573\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1209.40\n",
      "Num timesteps: 1207000; Episodes: 5576\n",
      "Best mean reward: 1217.04 - Last mean reward per episode: 1221.38\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1208000; Episodes: 5578\n",
      "Best mean reward: 1221.38 - Last mean reward per episode: 1223.20\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1209000; Episodes: 5582\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1214.68\n",
      "Num timesteps: 1210000; Episodes: 5585\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1200.87\n",
      "Saving new model file...\n",
      "Num timesteps: 1211000; Episodes: 5588\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1204.88\n",
      "Num timesteps: 1212000; Episodes: 5590\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1205.16\n",
      "Num timesteps: 1213000; Episodes: 5592\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1205.89\n",
      "Num timesteps: 1214000; Episodes: 5595\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1205.47\n",
      "Num timesteps: 1215000; Episodes: 5598\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1195.19\n",
      "Num timesteps: 1216000; Episodes: 5600\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1195.12\n",
      "Num timesteps: 1217000; Episodes: 5603\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1198.53\n",
      "Num timesteps: 1218000; Episodes: 5605\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1208.00\n",
      "Num timesteps: 1219000; Episodes: 5608\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1211.51\n",
      "Num timesteps: 1220000; Episodes: 5610\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1215.04\n",
      "Saving new model file...\n",
      "Num timesteps: 1221000; Episodes: 5614\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1206.25\n",
      "Num timesteps: 1222000; Episodes: 5616\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1203.37\n",
      "Num timesteps: 1223000; Episodes: 5619\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1215.07\n",
      "Num timesteps: 1224000; Episodes: 5621\n",
      "Best mean reward: 1223.20 - Last mean reward per episode: 1227.49\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1225000; Episodes: 5624\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1219.26\n",
      "Num timesteps: 1226000; Episodes: 5626\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1219.83\n",
      "Num timesteps: 1227000; Episodes: 5630\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1200.10\n",
      "Num timesteps: 1228000; Episodes: 5632\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1205.39\n",
      "Num timesteps: 1229000; Episodes: 5634\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1218.27\n",
      "Num timesteps: 1230000; Episodes: 5637\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1207.70\n",
      "Saving new model file...\n",
      "Num timesteps: 1231000; Episodes: 5640\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1204.62\n",
      "Num timesteps: 1232000; Episodes: 5642\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1204.76\n",
      "Num timesteps: 1233000; Episodes: 5644\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1208.67\n",
      "Num timesteps: 1234000; Episodes: 5647\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1202.55\n",
      "Num timesteps: 1235000; Episodes: 5651\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1178.18\n",
      "Num timesteps: 1236000; Episodes: 5653\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1181.94\n",
      "Num timesteps: 1237000; Episodes: 5655\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1194.31\n",
      "Num timesteps: 1238000; Episodes: 5658\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1195.54\n",
      "Num timesteps: 1239000; Episodes: 5660\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1195.98\n",
      "Num timesteps: 1240000; Episodes: 5664\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1182.50\n",
      "Saving new model file...\n",
      "Num timesteps: 1241000; Episodes: 5667\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1188.79\n",
      "Num timesteps: 1242000; Episodes: 5670\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1176.30\n",
      "Num timesteps: 1243000; Episodes: 5672\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1188.69\n",
      "Num timesteps: 1244000; Episodes: 5674\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1187.53\n",
      "Num timesteps: 1245000; Episodes: 5676\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1188.05\n",
      "Num timesteps: 1246000; Episodes: 5680\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1181.75\n",
      "Num timesteps: 1247000; Episodes: 5683\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1183.19\n",
      "Num timesteps: 1248000; Episodes: 5685\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1184.13\n",
      "Num timesteps: 1249000; Episodes: 5688\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1180.33\n",
      "Num timesteps: 1250000; Episodes: 5691\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1168.29\n",
      "Saving new model file...\n",
      "Num timesteps: 1251000; Episodes: 5693\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1169.62\n",
      "Num timesteps: 1252000; Episodes: 5696\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1159.30\n",
      "Num timesteps: 1253000; Episodes: 5699\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1166.90\n",
      "Num timesteps: 1254000; Episodes: 5701\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1176.13\n",
      "Num timesteps: 1255000; Episodes: 5705\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1161.44\n",
      "Num timesteps: 1256000; Episodes: 5707\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1157.00\n",
      "Num timesteps: 1257000; Episodes: 5710\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1156.98\n",
      "Num timesteps: 1258000; Episodes: 5712\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1151.19\n",
      "Num timesteps: 1259000; Episodes: 5714\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1163.68\n",
      "Num timesteps: 1260000; Episodes: 5717\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1172.64\n",
      "Saving new model file...\n",
      "Num timesteps: 1261000; Episodes: 5720\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1158.51\n",
      "Num timesteps: 1262000; Episodes: 5724\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1156.44\n",
      "Num timesteps: 1263000; Episodes: 5727\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1143.18\n",
      "Num timesteps: 1264000; Episodes: 5729\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1159.69\n",
      "Num timesteps: 1265000; Episodes: 5731\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1158.09\n",
      "Num timesteps: 1266000; Episodes: 5734\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1158.41\n",
      "Num timesteps: 1267000; Episodes: 5736\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1152.00\n",
      "Num timesteps: 1268000; Episodes: 5739\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1154.77\n",
      "Num timesteps: 1269000; Episodes: 5741\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1153.43\n",
      "Num timesteps: 1270000; Episodes: 5744\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1158.73\n",
      "Saving new model file...\n",
      "Num timesteps: 1271000; Episodes: 5746\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1159.00\n",
      "Num timesteps: 1272000; Episodes: 5748\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1167.27\n",
      "Num timesteps: 1273000; Episodes: 5751\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1191.42\n",
      "Num timesteps: 1274000; Episodes: 5754\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1185.47\n",
      "Num timesteps: 1275000; Episodes: 5756\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1186.41\n",
      "Num timesteps: 1276000; Episodes: 5759\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1180.07\n",
      "Num timesteps: 1277000; Episodes: 5761\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1181.49\n",
      "Num timesteps: 1278000; Episodes: 5763\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1191.10\n",
      "Num timesteps: 1279000; Episodes: 5766\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1198.06\n",
      "Num timesteps: 1280000; Episodes: 5769\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1198.25\n",
      "Saving new model file...\n",
      "Num timesteps: 1281000; Episodes: 5772\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1193.97\n",
      "Num timesteps: 1282000; Episodes: 5774\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1194.66\n",
      "Num timesteps: 1283000; Episodes: 5778\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1183.59\n",
      "Num timesteps: 1284000; Episodes: 5780\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1191.48\n",
      "Num timesteps: 1285000; Episodes: 5782\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1203.04\n",
      "Num timesteps: 1286000; Episodes: 5785\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1212.54\n",
      "Num timesteps: 1287000; Episodes: 5787\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1215.57\n",
      "Num timesteps: 1288000; Episodes: 5789\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1207.74\n",
      "Num timesteps: 1289000; Episodes: 5792\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1217.45\n",
      "Num timesteps: 1290000; Episodes: 5795\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1213.46\n",
      "Saving new model file...\n",
      "Num timesteps: 1291000; Episodes: 5797\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1222.40\n",
      "Num timesteps: 1292000; Episodes: 5800\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1210.64\n",
      "Num timesteps: 1293000; Episodes: 5803\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1195.87\n",
      "Num timesteps: 1294000; Episodes: 5807\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1198.58\n",
      "Num timesteps: 1295000; Episodes: 5809\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1203.85\n",
      "Num timesteps: 1296000; Episodes: 5812\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1204.77\n",
      "Num timesteps: 1297000; Episodes: 5814\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1201.02\n",
      "Num timesteps: 1298000; Episodes: 5817\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1193.57\n",
      "Num timesteps: 1299000; Episodes: 5820\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1205.26\n",
      "Num timesteps: 1300000; Episodes: 5824\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1194.38\n",
      "Saving new model file...\n",
      "Num timesteps: 1301000; Episodes: 5826\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1207.33\n",
      "Num timesteps: 1302000; Episodes: 5828\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1207.52\n",
      "Num timesteps: 1303000; Episodes: 5831\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1201.72\n",
      "Num timesteps: 1304000; Episodes: 5833\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1200.91\n",
      "Num timesteps: 1305000; Episodes: 5836\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1209.44\n",
      "Num timesteps: 1306000; Episodes: 5838\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1210.30\n",
      "Num timesteps: 1307000; Episodes: 5841\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1213.08\n",
      "Num timesteps: 1308000; Episodes: 5843\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1206.63\n",
      "Num timesteps: 1309000; Episodes: 5846\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1200.17\n",
      "Num timesteps: 1310000; Episodes: 5848\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1191.50\n",
      "Saving new model file...\n",
      "Num timesteps: 1311000; Episodes: 5851\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1191.23\n",
      "Num timesteps: 1312000; Episodes: 5854\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1189.49\n",
      "Num timesteps: 1313000; Episodes: 5857\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1181.22\n",
      "Num timesteps: 1314000; Episodes: 5859\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1185.53\n",
      "Num timesteps: 1315000; Episodes: 5862\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1187.76\n",
      "Num timesteps: 1316000; Episodes: 5865\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1176.23\n",
      "Num timesteps: 1317000; Episodes: 5867\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1180.76\n",
      "Num timesteps: 1318000; Episodes: 5870\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1183.14\n",
      "Num timesteps: 1319000; Episodes: 5874\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1163.81\n",
      "Num timesteps: 1320000; Episodes: 5877\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1165.33\n",
      "Saving new model file...\n",
      "Num timesteps: 1321000; Episodes: 5879\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1157.43\n",
      "Num timesteps: 1322000; Episodes: 5880\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1133.09\n",
      "Num timesteps: 1323000; Episodes: 5882\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1125.72\n",
      "Num timesteps: 1324000; Episodes: 5885\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1130.30\n",
      "Num timesteps: 1325000; Episodes: 5887\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1131.01\n",
      "Num timesteps: 1326000; Episodes: 5889\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1137.85\n",
      "Num timesteps: 1327000; Episodes: 5892\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1129.03\n",
      "Num timesteps: 1328000; Episodes: 5895\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1128.67\n",
      "Num timesteps: 1329000; Episodes: 5898\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1122.15\n",
      "Num timesteps: 1330000; Episodes: 5900\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1131.44\n",
      "Saving new model file...\n",
      "Num timesteps: 1331000; Episodes: 5902\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1143.86\n",
      "Num timesteps: 1332000; Episodes: 5904\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1150.52\n",
      "Num timesteps: 1333000; Episodes: 5906\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1155.98\n",
      "Num timesteps: 1334000; Episodes: 5909\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1162.20\n",
      "Num timesteps: 1335000; Episodes: 5911\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1158.10\n",
      "Num timesteps: 1336000; Episodes: 5914\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1169.46\n",
      "Num timesteps: 1337000; Episodes: 5916\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1168.37\n",
      "Num timesteps: 1338000; Episodes: 5919\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1185.41\n",
      "Num timesteps: 1339000; Episodes: 5923\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1182.84\n",
      "Num timesteps: 1340000; Episodes: 5927\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1155.40\n",
      "Saving new model file...\n",
      "Num timesteps: 1341000; Episodes: 5930\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1149.55\n",
      "Num timesteps: 1342000; Episodes: 5932\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1146.61\n",
      "Num timesteps: 1343000; Episodes: 5935\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1140.35\n",
      "Num timesteps: 1344000; Episodes: 5937\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1145.18\n",
      "Num timesteps: 1345000; Episodes: 5940\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1139.41\n",
      "Num timesteps: 1346000; Episodes: 5943\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1146.67\n",
      "Num timesteps: 1347000; Episodes: 5945\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1146.58\n",
      "Num timesteps: 1348000; Episodes: 5947\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1155.58\n",
      "Num timesteps: 1349000; Episodes: 5950\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1151.51\n",
      "Num timesteps: 1350000; Episodes: 5952\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1151.72\n",
      "Saving new model file...\n",
      "Num timesteps: 1351000; Episodes: 5955\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1158.68\n",
      "Num timesteps: 1352000; Episodes: 5957\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1170.36\n",
      "Num timesteps: 1353000; Episodes: 5959\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1170.35\n",
      "Num timesteps: 1354000; Episodes: 5962\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1163.87\n",
      "Num timesteps: 1355000; Episodes: 5964\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1173.59\n",
      "Num timesteps: 1356000; Episodes: 5967\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1181.04\n",
      "Num timesteps: 1357000; Episodes: 5969\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1181.48\n",
      "Num timesteps: 1358000; Episodes: 5972\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1199.73\n",
      "Num timesteps: 1359000; Episodes: 5974\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1208.27\n",
      "Num timesteps: 1360000; Episodes: 5976\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1217.95\n",
      "Saving new model file...\n",
      "Num timesteps: 1361000; Episodes: 5978\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1218.25\n",
      "Num timesteps: 1362000; Episodes: 5981\n",
      "Best mean reward: 1227.49 - Last mean reward per episode: 1254.81\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1363000; Episodes: 5983\n",
      "Best mean reward: 1254.81 - Last mean reward per episode: 1254.68\n",
      "Num timesteps: 1364000; Episodes: 5985\n",
      "Best mean reward: 1254.81 - Last mean reward per episode: 1253.73\n",
      "Num timesteps: 1365000; Episodes: 5988\n",
      "Best mean reward: 1254.81 - Last mean reward per episode: 1247.29\n",
      "Num timesteps: 1366000; Episodes: 5991\n",
      "Best mean reward: 1254.81 - Last mean reward per episode: 1247.13\n",
      "Num timesteps: 1367000; Episodes: 5994\n",
      "Best mean reward: 1254.81 - Last mean reward per episode: 1239.97\n",
      "Num timesteps: 1368000; Episodes: 5996\n",
      "Best mean reward: 1254.81 - Last mean reward per episode: 1259.63\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Num timesteps: 1369000; Episodes: 5999\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1246.20\n",
      "Num timesteps: 1370000; Episodes: 6004\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1218.77\n",
      "Saving new model file...\n",
      "Num timesteps: 1371000; Episodes: 6006\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1220.83\n",
      "Num timesteps: 1372000; Episodes: 6008\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1216.58\n",
      "Num timesteps: 1373000; Episodes: 6011\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1223.72\n",
      "Num timesteps: 1374000; Episodes: 6013\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1223.97\n",
      "Num timesteps: 1375000; Episodes: 6015\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1224.69\n",
      "Num timesteps: 1376000; Episodes: 6019\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1206.11\n",
      "Num timesteps: 1377000; Episodes: 6021\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1208.30\n",
      "Num timesteps: 1378000; Episodes: 6024\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1230.59\n",
      "Num timesteps: 1379000; Episodes: 6026\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1242.24\n",
      "Num timesteps: 1380000; Episodes: 6029\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1250.18\n",
      "Saving new model file...\n",
      "Num timesteps: 1381000; Episodes: 6032\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1252.19\n",
      "Num timesteps: 1382000; Episodes: 6034\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1256.73\n",
      "Num timesteps: 1383000; Episodes: 6036\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1256.09\n",
      "Num timesteps: 1384000; Episodes: 6039\n",
      "Best mean reward: 1259.63 - Last mean reward per episode: 1260.67\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 247.02777256922482 \n",
      "Num timesteps: 1385000; Episodes: 6041\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1259.89\n",
      "Num timesteps: 1386000; Episodes: 6045\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1245.06\n",
      "Num timesteps: 1387000; Episodes: 6047\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1241.56\n",
      "Num timesteps: 1388000; Episodes: 6049\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1240.95\n",
      "Num timesteps: 1389000; Episodes: 6052\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1242.79\n",
      "Num timesteps: 1390000; Episodes: 6054\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1246.52\n",
      "Saving new model file...\n",
      "Num timesteps: 1391000; Episodes: 6057\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1229.43\n",
      "Num timesteps: 1392000; Episodes: 6061\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1207.52\n",
      "Num timesteps: 1393000; Episodes: 6063\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1219.94\n",
      "Num timesteps: 1394000; Episodes: 6065\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1221.61\n",
      "Num timesteps: 1395000; Episodes: 6068\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1214.77\n",
      "Num timesteps: 1396000; Episodes: 6072\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1185.05\n",
      "Num timesteps: 1397000; Episodes: 6076\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1175.50\n",
      "Num timesteps: 1398000; Episodes: 6079\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1165.38\n",
      "Num timesteps: 1399000; Episodes: 6081\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1167.89\n",
      "Num timesteps: 1400000; Episodes: 6084\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1167.09\n",
      "Saving new model file...\n",
      "Num timesteps: 1401000; Episodes: 6086\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1168.04\n",
      "Num timesteps: 1402000; Episodes: 6089\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1164.93\n",
      "Num timesteps: 1403000; Episodes: 6092\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1164.43\n",
      "Num timesteps: 1404000; Episodes: 6096\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1144.99\n",
      "Num timesteps: 1405000; Episodes: 6098\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1143.19\n",
      "Num timesteps: 1406000; Episodes: 6100\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1157.04\n",
      "Num timesteps: 1407000; Episodes: 6103\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1177.52\n",
      "Num timesteps: 1408000; Episodes: 6105\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1185.98\n",
      "Num timesteps: 1409000; Episodes: 6107\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1186.90\n",
      "Num timesteps: 1410000; Episodes: 6111\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1176.37\n",
      "Saving new model file...\n",
      "Num timesteps: 1411000; Episodes: 6113\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1168.23\n",
      "Num timesteps: 1412000; Episodes: 6116\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1167.93\n",
      "Num timesteps: 1413000; Episodes: 6118\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1176.76\n",
      "Num timesteps: 1414000; Episodes: 6120\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1187.94\n",
      "Num timesteps: 1415000; Episodes: 6122\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1187.74\n",
      "Num timesteps: 1416000; Episodes: 6125\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1191.87\n",
      "Num timesteps: 1417000; Episodes: 6129\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1175.23\n",
      "Num timesteps: 1418000; Episodes: 6132\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1177.55\n",
      "Num timesteps: 1419000; Episodes: 6134\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1176.70\n",
      "Num timesteps: 1420000; Episodes: 6136\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1177.17\n",
      "Saving new model file...\n",
      "Num timesteps: 1421000; Episodes: 6139\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1181.15\n",
      "Num timesteps: 1422000; Episodes: 6141\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1180.96\n",
      "Num timesteps: 1423000; Episodes: 6143\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1188.54\n",
      "Num timesteps: 1424000; Episodes: 6146\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1198.18\n",
      "Num timesteps: 1425000; Episodes: 6149\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1183.45\n",
      "Num timesteps: 1426000; Episodes: 6151\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1186.14\n",
      "Num timesteps: 1427000; Episodes: 6155\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1172.41\n",
      "Num timesteps: 1428000; Episodes: 6158\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1175.87\n",
      "Num timesteps: 1429000; Episodes: 6160\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1185.32\n",
      "Num timesteps: 1430000; Episodes: 6163\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1197.30\n",
      "Saving new model file...\n",
      "Num timesteps: 1431000; Episodes: 6166\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1175.20\n",
      "Num timesteps: 1432000; Episodes: 6169\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1176.16\n",
      "Num timesteps: 1433000; Episodes: 6171\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1198.11\n",
      "Num timesteps: 1434000; Episodes: 6174\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1212.85\n",
      "Num timesteps: 1435000; Episodes: 6177\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1218.32\n",
      "Num timesteps: 1436000; Episodes: 6179\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1217.00\n",
      "Num timesteps: 1437000; Episodes: 6182\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1203.49\n",
      "Num timesteps: 1438000; Episodes: 6185\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1199.71\n",
      "Num timesteps: 1439000; Episodes: 6187\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1209.89\n",
      "Num timesteps: 1440000; Episodes: 6190\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1215.14\n",
      "Saving new model file...\n",
      "Num timesteps: 1441000; Episodes: 6192\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1219.32\n",
      "Num timesteps: 1442000; Episodes: 6195\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1235.72\n",
      "Num timesteps: 1443000; Episodes: 6197\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1231.19\n",
      "Num timesteps: 1444000; Episodes: 6200\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1232.53\n",
      "Num timesteps: 1445000; Episodes: 6202\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1233.20\n",
      "Num timesteps: 1446000; Episodes: 6205\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1229.83\n",
      "Num timesteps: 1447000; Episodes: 6207\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1222.78\n",
      "Num timesteps: 1448000; Episodes: 6210\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1230.35\n",
      "Num timesteps: 1449000; Episodes: 6212\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1229.19\n",
      "Num timesteps: 1450000; Episodes: 6215\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1237.64\n",
      "Saving new model file...\n",
      "Num timesteps: 1451000; Episodes: 6217\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1231.83\n",
      "Num timesteps: 1452000; Episodes: 6220\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1232.38\n",
      "Num timesteps: 1453000; Episodes: 6224\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1211.43\n",
      "Num timesteps: 1454000; Episodes: 6226\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1216.70\n",
      "Num timesteps: 1455000; Episodes: 6228\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1229.04\n",
      "Num timesteps: 1456000; Episodes: 6231\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1240.29\n",
      "Num timesteps: 1457000; Episodes: 6233\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1243.82\n",
      "Num timesteps: 1458000; Episodes: 6235\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1243.47\n",
      "Num timesteps: 1459000; Episodes: 6239\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1226.26\n",
      "Num timesteps: 1460000; Episodes: 6241\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1218.53\n",
      "Saving new model file...\n",
      "Num timesteps: 1461000; Episodes: 6244\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1213.97\n",
      "Num timesteps: 1462000; Episodes: 6247\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1204.20\n",
      "Num timesteps: 1463000; Episodes: 6249\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1223.02\n",
      "Num timesteps: 1464000; Episodes: 6252\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1222.77\n",
      "Num timesteps: 1465000; Episodes: 6254\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1228.63\n",
      "Num timesteps: 1466000; Episodes: 6257\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1235.84\n",
      "Num timesteps: 1467000; Episodes: 6259\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1243.14\n",
      "Num timesteps: 1468000; Episodes: 6262\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1231.73\n",
      "Num timesteps: 1469000; Episodes: 6265\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1243.72\n",
      "Num timesteps: 1470000; Episodes: 6268\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1239.46\n",
      "Saving new model file...\n",
      "Num timesteps: 1471000; Episodes: 6271\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1237.84\n",
      "Num timesteps: 1472000; Episodes: 6273\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1237.67\n",
      "Num timesteps: 1473000; Episodes: 6276\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1247.63\n",
      "Num timesteps: 1474000; Episodes: 6278\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1250.15\n",
      "Num timesteps: 1475000; Episodes: 6280\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1251.05\n",
      "Num timesteps: 1476000; Episodes: 6283\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1257.85\n",
      "Num timesteps: 1477000; Episodes: 6286\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1255.81\n",
      "Num timesteps: 1478000; Episodes: 6289\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1249.41\n",
      "Num timesteps: 1479000; Episodes: 6292\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1241.56\n",
      "Num timesteps: 1480000; Episodes: 6294\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1245.92\n",
      "Saving new model file...\n",
      "Num timesteps: 1481000; Episodes: 6296\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1250.56\n",
      "Num timesteps: 1482000; Episodes: 6299\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1247.13\n",
      "Num timesteps: 1483000; Episodes: 6302\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1235.25\n",
      "Num timesteps: 1484000; Episodes: 6305\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1234.59\n",
      "Num timesteps: 1485000; Episodes: 6307\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1241.52\n",
      "Num timesteps: 1486000; Episodes: 6309\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1241.18\n",
      "Num timesteps: 1487000; Episodes: 6312\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1244.98\n",
      "Num timesteps: 1488000; Episodes: 6315\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1233.18\n",
      "Num timesteps: 1489000; Episodes: 6317\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1239.13\n",
      "Num timesteps: 1490000; Episodes: 6320\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1239.79\n",
      "Saving new model file...\n",
      "Num timesteps: 1491000; Episodes: 6322\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1249.99\n",
      "Num timesteps: 1492000; Episodes: 6325\n",
      "Best mean reward: 1260.67 - Last mean reward per episode: 1267.72\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 231.294653155509 \n",
      "Num timesteps: 1493000; Episodes: 6327\n",
      "Best mean reward: 1267.72 - Last mean reward per episode: 1268.23\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 224.49350297682864 \n",
      "Num timesteps: 1494000; Episodes: 6329\n",
      "Best mean reward: 1268.23 - Last mean reward per episode: 1268.46\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 295.9896881470213 \n",
      "Num timesteps: 1495000; Episodes: 6332\n",
      "Best mean reward: 1268.46 - Last mean reward per episode: 1264.81\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 280.84218561926986 \n",
      "Num timesteps: 1496000; Episodes: 6334\n",
      "Best mean reward: 1268.46 - Last mean reward per episode: 1261.89\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 256.03948786620373 \n",
      "Num timesteps: 1497000; Episodes: 6336\n",
      "Best mean reward: 1268.46 - Last mean reward per episode: 1259.93\n",
      "Num timesteps: 1498000; Episodes: 6338\n",
      "Best mean reward: 1268.46 - Last mean reward per episode: 1264.57\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 294.2874055545136 \n",
      "Num timesteps: 1499000; Episodes: 6340\n",
      "Best mean reward: 1268.46 - Last mean reward per episode: 1276.49\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 297.89452982351287 \n",
      "Num timesteps: 1500000; Episodes: 6343\n",
      "Best mean reward: 1276.49 - Last mean reward per episode: 1284.98\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 205.58654535925854 \n",
      "Saving new model file...\n",
      "Num timesteps: 1501000; Episodes: 6346\n",
      "Best mean reward: 1284.98 - Last mean reward per episode: 1284.26\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 148.19859960782625 \n",
      "Num timesteps: 1502000; Episodes: 6348\n",
      "Best mean reward: 1284.98 - Last mean reward per episode: 1287.30\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 293.0745484660908 \n",
      "Num timesteps: 1503000; Episodes: 6351\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1277.80\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 294.84128521206947 \n",
      "Num timesteps: 1504000; Episodes: 6354\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1271.27\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 247.48385924271733 \n",
      "Num timesteps: 1505000; Episodes: 6356\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1271.53\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 238.5742995598887 \n",
      "Num timesteps: 1506000; Episodes: 6359\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1273.12\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 262.52154496844656 \n",
      "Num timesteps: 1507000; Episodes: 6361\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1280.72\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 261.0426194396228 \n",
      "Num timesteps: 1508000; Episodes: 6363\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1280.26\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 204.94320541739984 \n",
      "Num timesteps: 1509000; Episodes: 6366\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1276.74\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 284.8669071163918 \n",
      "Num timesteps: 1510000; Episodes: 6369\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1280.20\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 295.5725090928045 \n",
      "Saving new model file...\n",
      "Num timesteps: 1511000; Episodes: 6373\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1273.42\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 193.9319590141301 \n",
      "Num timesteps: 1512000; Episodes: 6375\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1273.75\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 287.8219640524465 \n",
      "Num timesteps: 1513000; Episodes: 6377\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1273.25\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 193.37861997421342 \n",
      "Num timesteps: 1514000; Episodes: 6380\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1264.55\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 296.0851150777149 \n",
      "Num timesteps: 1515000; Episodes: 6383\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1276.98\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 269.23843350662673 \n",
      "Num timesteps: 1516000; Episodes: 6385\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1279.57\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 276.4597551355995 \n",
      "Num timesteps: 1517000; Episodes: 6387\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1278.83\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 273.53493453997925 \n",
      "Num timesteps: 1518000; Episodes: 6389\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1286.00\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 207.18158670727175 \n",
      "Num timesteps: 1519000; Episodes: 6392\n",
      "Best mean reward: 1287.30 - Last mean reward per episode: 1296.47\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 280.7601590238957 \n",
      "Num timesteps: 1520000; Episodes: 6395\n",
      "Best mean reward: 1296.47 - Last mean reward per episode: 1289.07\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 272.60020410392883 \n",
      "Saving new model file...\n",
      "Num timesteps: 1521000; Episodes: 6397\n",
      "Best mean reward: 1296.47 - Last mean reward per episode: 1288.74\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 278.9154173772672 \n",
      "Num timesteps: 1522000; Episodes: 6399\n",
      "Best mean reward: 1296.47 - Last mean reward per episode: 1297.09\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 236.786311666263 \n",
      "Num timesteps: 1523000; Episodes: 6402\n",
      "Best mean reward: 1297.09 - Last mean reward per episode: 1301.77\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 288.03225407789716 \n",
      "Num timesteps: 1524000; Episodes: 6405\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1298.17\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 289.4393811151387 \n",
      "Num timesteps: 1525000; Episodes: 6409\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1275.28\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 278.9683561292644 \n",
      "Num timesteps: 1526000; Episodes: 6412\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1273.30\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 231.77982089168108 \n",
      "Num timesteps: 1527000; Episodes: 6414\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1272.57\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 274.3823726722218 \n",
      "Num timesteps: 1528000; Episodes: 6417\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1276.26\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 206.77694257730798 \n",
      "Num timesteps: 1529000; Episodes: 6419\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1275.43\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 259.2280086739658 \n",
      "Num timesteps: 1530000; Episodes: 6422\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1273.15\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 161.32689601391087 \n",
      "Saving new model file...\n",
      "Num timesteps: 1531000; Episodes: 6424\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1272.84\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 229.22062876526914 \n",
      "Num timesteps: 1532000; Episodes: 6427\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1268.54\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 189.71765057046989 \n",
      "Num timesteps: 1533000; Episodes: 6429\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1258.69\n",
      "Num timesteps: 1534000; Episodes: 6432\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1262.70\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 147.6576892333211 \n",
      "Num timesteps: 1535000; Episodes: 6435\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1258.20\n",
      "Num timesteps: 1536000; Episodes: 6438\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1250.60\n",
      "Num timesteps: 1537000; Episodes: 6440\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1249.71\n",
      "Num timesteps: 1538000; Episodes: 6443\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1243.45\n",
      "Num timesteps: 1539000; Episodes: 6445\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1252.82\n",
      "Num timesteps: 1540000; Episodes: 6447\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1258.74\n",
      "Saving new model file...\n",
      "Num timesteps: 1541000; Episodes: 6450\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1268.45\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 280.388174334019 \n",
      "Num timesteps: 1542000; Episodes: 6452\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1269.72\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 282.7281944866061 \n",
      "Num timesteps: 1543000; Episodes: 6454\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1279.45\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 265.7234691615436 \n",
      "Num timesteps: 1544000; Episodes: 6456\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1278.72\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 291.62902289430605 \n",
      "Num timesteps: 1545000; Episodes: 6460\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1261.11\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 229.0053953560338 \n",
      "Num timesteps: 1546000; Episodes: 6462\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1263.77\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 226.45404863815762 \n",
      "Num timesteps: 1547000; Episodes: 6465\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1260.13\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 189.26232862451806 \n",
      "Num timesteps: 1548000; Episodes: 6468\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1273.02\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 171.90361083875564 \n",
      "Num timesteps: 1549000; Episodes: 6470\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1273.31\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.2844560290193 \n",
      "Num timesteps: 1550000; Episodes: 6474\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1267.17\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 287.22358308893405 \n",
      "Saving new model file...\n",
      "Num timesteps: 1551000; Episodes: 6478\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1235.60\n",
      "Num timesteps: 1552000; Episodes: 6481\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1239.96\n",
      "Num timesteps: 1553000; Episodes: 6483\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1239.55\n",
      "Num timesteps: 1554000; Episodes: 6486\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1229.83\n",
      "Num timesteps: 1555000; Episodes: 6489\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1219.23\n",
      "Num timesteps: 1556000; Episodes: 6491\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1219.14\n",
      "Num timesteps: 1557000; Episodes: 6493\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1227.23\n",
      "Num timesteps: 1558000; Episodes: 6496\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1214.66\n",
      "Num timesteps: 1559000; Episodes: 6499\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1214.87\n",
      "Num timesteps: 1560000; Episodes: 6502\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1214.73\n",
      "Saving new model file...\n",
      "Num timesteps: 1561000; Episodes: 6504\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1223.98\n",
      "Num timesteps: 1562000; Episodes: 6506\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1224.13\n",
      "Num timesteps: 1563000; Episodes: 6510\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1238.32\n",
      "Num timesteps: 1564000; Episodes: 6512\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1237.21\n",
      "Num timesteps: 1565000; Episodes: 6515\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1229.99\n",
      "Num timesteps: 1566000; Episodes: 6518\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1227.30\n",
      "Num timesteps: 1567000; Episodes: 6520\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1226.40\n",
      "Num timesteps: 1568000; Episodes: 6522\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1228.24\n",
      "Num timesteps: 1569000; Episodes: 6526\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1206.43\n",
      "Num timesteps: 1570000; Episodes: 6528\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1208.46\n",
      "Saving new model file...\n",
      "Num timesteps: 1571000; Episodes: 6531\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1209.67\n",
      "Num timesteps: 1572000; Episodes: 6533\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1214.16\n",
      "Num timesteps: 1573000; Episodes: 6535\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1217.32\n",
      "Num timesteps: 1574000; Episodes: 6538\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1217.85\n",
      "Num timesteps: 1575000; Episodes: 6541\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1209.47\n",
      "Num timesteps: 1576000; Episodes: 6543\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1215.95\n",
      "Num timesteps: 1577000; Episodes: 6545\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1216.15\n",
      "Num timesteps: 1578000; Episodes: 6549\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1197.73\n",
      "Num timesteps: 1579000; Episodes: 6552\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1190.17\n",
      "Num timesteps: 1580000; Episodes: 6554\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1191.17\n",
      "Saving new model file...\n",
      "Num timesteps: 1581000; Episodes: 6557\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1179.07\n",
      "Num timesteps: 1582000; Episodes: 6559\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1196.43\n",
      "Num timesteps: 1583000; Episodes: 6562\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1199.33\n",
      "Num timesteps: 1584000; Episodes: 6565\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1196.24\n",
      "Num timesteps: 1585000; Episodes: 6568\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1177.57\n",
      "Num timesteps: 1586000; Episodes: 6570\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1177.97\n",
      "Num timesteps: 1587000; Episodes: 6574\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1187.02\n",
      "Num timesteps: 1588000; Episodes: 6577\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1203.35\n",
      "Num timesteps: 1589000; Episodes: 6579\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1209.92\n",
      "Num timesteps: 1590000; Episodes: 6582\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1208.17\n",
      "Saving new model file...\n",
      "Num timesteps: 1591000; Episodes: 6584\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1203.54\n",
      "Num timesteps: 1592000; Episodes: 6587\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1217.63\n",
      "Num timesteps: 1593000; Episodes: 6589\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1219.36\n",
      "Num timesteps: 1594000; Episodes: 6592\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1212.84\n",
      "Num timesteps: 1595000; Episodes: 6596\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1211.46\n",
      "Num timesteps: 1596000; Episodes: 6598\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1211.50\n",
      "Num timesteps: 1597000; Episodes: 6601\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1206.96\n",
      "Num timesteps: 1598000; Episodes: 6603\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1202.97\n",
      "Num timesteps: 1599000; Episodes: 6607\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1190.56\n",
      "Num timesteps: 1600000; Episodes: 6610\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1200.41\n",
      "Saving new model file...\n",
      "Num timesteps: 1601000; Episodes: 6612\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1200.97\n",
      "Num timesteps: 1602000; Episodes: 6614\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1201.19\n",
      "Num timesteps: 1603000; Episodes: 6617\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1212.18\n",
      "Num timesteps: 1604000; Episodes: 6619\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1212.24\n",
      "Num timesteps: 1605000; Episodes: 6621\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1212.48\n",
      "Num timesteps: 1606000; Episodes: 6623\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1217.72\n",
      "Num timesteps: 1607000; Episodes: 6627\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1213.94\n",
      "Num timesteps: 1608000; Episodes: 6629\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1213.98\n",
      "Num timesteps: 1609000; Episodes: 6632\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1217.98\n",
      "Num timesteps: 1610000; Episodes: 6634\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1217.65\n",
      "Saving new model file...\n",
      "Num timesteps: 1611000; Episodes: 6637\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1207.57\n",
      "Num timesteps: 1612000; Episodes: 6640\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1213.59\n",
      "Num timesteps: 1613000; Episodes: 6643\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1209.16\n",
      "Num timesteps: 1614000; Episodes: 6645\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1208.61\n",
      "Num timesteps: 1615000; Episodes: 6647\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1219.75\n",
      "Num timesteps: 1616000; Episodes: 6650\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1226.73\n",
      "Num timesteps: 1617000; Episodes: 6652\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1233.75\n",
      "Num timesteps: 1618000; Episodes: 6654\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1232.75\n",
      "Num timesteps: 1619000; Episodes: 6657\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1239.87\n",
      "Num timesteps: 1620000; Episodes: 6659\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1239.33\n",
      "Saving new model file...\n",
      "Num timesteps: 1621000; Episodes: 6663\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1219.93\n",
      "Num timesteps: 1622000; Episodes: 6666\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1231.93\n",
      "Num timesteps: 1623000; Episodes: 6668\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1236.83\n",
      "Num timesteps: 1624000; Episodes: 6670\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1232.87\n",
      "Num timesteps: 1625000; Episodes: 6673\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1234.76\n",
      "Num timesteps: 1626000; Episodes: 6676\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1243.75\n",
      "Num timesteps: 1627000; Episodes: 6678\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1249.04\n",
      "Num timesteps: 1628000; Episodes: 6681\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1239.08\n",
      "Num timesteps: 1629000; Episodes: 6684\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1238.28\n",
      "Num timesteps: 1630000; Episodes: 6686\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1238.31\n",
      "Saving new model file...\n",
      "Num timesteps: 1631000; Episodes: 6688\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1246.03\n",
      "Num timesteps: 1632000; Episodes: 6692\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1236.32\n",
      "Num timesteps: 1633000; Episodes: 6694\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1232.05\n",
      "Num timesteps: 1634000; Episodes: 6696\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1245.62\n",
      "Num timesteps: 1635000; Episodes: 6699\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1245.28\n",
      "Num timesteps: 1636000; Episodes: 6701\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1254.93\n",
      "Num timesteps: 1637000; Episodes: 6704\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1258.13\n",
      "Num timesteps: 1638000; Episodes: 6706\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1258.27\n",
      "Num timesteps: 1639000; Episodes: 6709\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1273.70\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 269.9661774200441 \n",
      "Num timesteps: 1640000; Episodes: 6711\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1273.69\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 150.78839215629134 \n",
      "Saving new model file...\n",
      "Num timesteps: 1641000; Episodes: 6713\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1272.71\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 260.08129600120685 \n",
      "Num timesteps: 1642000; Episodes: 6716\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1266.64\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 191.4143668965902 \n",
      "Num timesteps: 1643000; Episodes: 6719\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1264.48\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 223.77724622274144 \n",
      "Num timesteps: 1644000; Episodes: 6721\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1263.95\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 184.61133040806 \n",
      "Num timesteps: 1645000; Episodes: 6723\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1264.59\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 267.17574698068967 \n",
      "Num timesteps: 1646000; Episodes: 6726\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1274.96\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 285.80902223427904 \n",
      "Num timesteps: 1647000; Episodes: 6728\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1288.24\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 260.89585667505867 \n",
      "Num timesteps: 1648000; Episodes: 6730\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1288.57\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 268.2072763113593 \n",
      "Num timesteps: 1649000; Episodes: 6733\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1291.73\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 260.6335081331929 \n",
      "Num timesteps: 1650000; Episodes: 6735\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1291.92\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 245.74187769809876 \n",
      "Saving new model file...\n",
      "Num timesteps: 1651000; Episodes: 6737\n",
      "Best mean reward: 1301.77 - Last mean reward per episode: 1302.33\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 252.51922933321723 \n",
      "Num timesteps: 1652000; Episodes: 6740\n",
      "Best mean reward: 1302.33 - Last mean reward per episode: 1304.23\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 284.0943188743378 \n",
      "Num timesteps: 1653000; Episodes: 6743\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1302.02\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 260.92817501679883 \n",
      "Num timesteps: 1654000; Episodes: 6746\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1288.20\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 285.1873563480383 \n",
      "Num timesteps: 1655000; Episodes: 6749\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1288.51\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 288.48070060425044 \n",
      "Num timesteps: 1656000; Episodes: 6751\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1276.91\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 269.7974824794161 \n",
      "Num timesteps: 1657000; Episodes: 6754\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1266.49\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 292.9644878179504 \n",
      "Num timesteps: 1658000; Episodes: 6757\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1267.10\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 286.21675582126335 \n",
      "Num timesteps: 1659000; Episodes: 6759\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1270.75\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 299.4416688651708 \n",
      "Num timesteps: 1660000; Episodes: 6761\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1283.26\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 282.279440244287 \n",
      "Saving new model file...\n",
      "Num timesteps: 1661000; Episodes: 6764\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1285.90\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 182.2008001936798 \n",
      "Num timesteps: 1662000; Episodes: 6767\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1286.66\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 109.78240578204763 \n",
      "Num timesteps: 1663000; Episodes: 6769\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1287.09\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 249.77802022881176 \n",
      "Num timesteps: 1664000; Episodes: 6772\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1300.75\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 264.125894511917 \n",
      "Num timesteps: 1665000; Episodes: 6774\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1292.30\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 256.7529602481278 \n",
      "Num timesteps: 1666000; Episodes: 6777\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1292.69\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 260.29359856753507 \n",
      "Num timesteps: 1667000; Episodes: 6779\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1292.22\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 291.03138706277 \n",
      "Num timesteps: 1668000; Episodes: 6782\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1292.27\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 283.8657624428454 \n",
      "Num timesteps: 1669000; Episodes: 6784\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1297.15\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 186.6438888872365 \n",
      "Num timesteps: 1670000; Episodes: 6787\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1296.37\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 242.99724075127307 \n",
      "Saving new model file...\n",
      "Num timesteps: 1671000; Episodes: 6789\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1297.72\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 293.5010372729303 \n",
      "Num timesteps: 1672000; Episodes: 6792\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1302.90\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 284.4734882684537 \n",
      "Num timesteps: 1673000; Episodes: 6794\n",
      "Best mean reward: 1304.23 - Last mean reward per episode: 1307.46\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 273.28049485972207 \n",
      "Num timesteps: 1674000; Episodes: 6797\n",
      "Best mean reward: 1307.46 - Last mean reward per episode: 1303.95\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 292.0843927821191 \n",
      "Num timesteps: 1675000; Episodes: 6800\n",
      "Best mean reward: 1307.46 - Last mean reward per episode: 1303.93\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 224.4842712020387 \n",
      "Num timesteps: 1676000; Episodes: 6802\n",
      "Best mean reward: 1307.46 - Last mean reward per episode: 1305.36\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.41374195995695 \n",
      "Num timesteps: 1677000; Episodes: 6805\n",
      "Best mean reward: 1307.46 - Last mean reward per episode: 1305.53\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 270.27637219915016 \n",
      "Num timesteps: 1678000; Episodes: 6807\n",
      "Best mean reward: 1307.46 - Last mean reward per episode: 1311.23\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 254.92028462057692 \n",
      "Num timesteps: 1679000; Episodes: 6809\n",
      "Best mean reward: 1311.23 - Last mean reward per episode: 1311.35\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 147.35925467613018 \n",
      "Num timesteps: 1680000; Episodes: 6812\n",
      "Best mean reward: 1311.35 - Last mean reward per episode: 1311.85\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 187.90413983326718 \n",
      "Saving new model file...\n",
      "Num timesteps: 1681000; Episodes: 6815\n",
      "Best mean reward: 1311.85 - Last mean reward per episode: 1310.30\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 228.24971184035837 \n",
      "Num timesteps: 1682000; Episodes: 6817\n",
      "Best mean reward: 1311.85 - Last mean reward per episode: 1314.51\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 296.1086715531871 \n",
      "Num timesteps: 1683000; Episodes: 6819\n",
      "Best mean reward: 1314.51 - Last mean reward per episode: 1320.68\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 223.65251132149584 \n",
      "Num timesteps: 1684000; Episodes: 6822\n",
      "Best mean reward: 1320.68 - Last mean reward per episode: 1320.60\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 143.7152994896688 \n",
      "Num timesteps: 1685000; Episodes: 6824\n",
      "Best mean reward: 1320.68 - Last mean reward per episode: 1320.78\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 278.5587496535389 \n",
      "Num timesteps: 1686000; Episodes: 6827\n",
      "Best mean reward: 1320.78 - Last mean reward per episode: 1319.74\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 152.72027195433094 \n",
      "Num timesteps: 1687000; Episodes: 6829\n",
      "Best mean reward: 1320.78 - Last mean reward per episode: 1319.49\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 274.60954970068497 \n",
      "Num timesteps: 1688000; Episodes: 6831\n",
      "Best mean reward: 1320.78 - Last mean reward per episode: 1319.32\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 173.68688920919232 \n",
      "Num timesteps: 1689000; Episodes: 6834\n",
      "Best mean reward: 1320.78 - Last mean reward per episode: 1319.03\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 243.524095245778 \n",
      "Num timesteps: 1690000; Episodes: 6836\n",
      "Best mean reward: 1320.78 - Last mean reward per episode: 1318.77\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 287.5921967087046 \n",
      "Saving new model file...\n",
      "Num timesteps: 1691000; Episodes: 6838\n",
      "Best mean reward: 1320.78 - Last mean reward per episode: 1318.12\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 231.10791499084 \n",
      "Num timesteps: 1692000; Episodes: 6841\n",
      "Best mean reward: 1320.78 - Last mean reward per episode: 1324.47\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 270.44318141140116 \n",
      "Num timesteps: 1693000; Episodes: 6844\n",
      "Best mean reward: 1324.47 - Last mean reward per episode: 1318.76\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 253.78747798910163 \n",
      "Num timesteps: 1694000; Episodes: 6847\n",
      "Best mean reward: 1324.47 - Last mean reward per episode: 1325.81\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 295.41393444655273 \n",
      "Num timesteps: 1695000; Episodes: 6849\n",
      "Best mean reward: 1325.81 - Last mean reward per episode: 1325.65\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 298.9601053126737 \n",
      "Num timesteps: 1696000; Episodes: 6851\n",
      "Best mean reward: 1325.81 - Last mean reward per episode: 1337.00\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.5147631350797 \n",
      "Num timesteps: 1697000; Episodes: 6854\n",
      "Best mean reward: 1337.00 - Last mean reward per episode: 1345.31\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.621037629096 \n",
      "Num timesteps: 1698000; Episodes: 6857\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1335.04\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 294.580470121233 \n",
      "Num timesteps: 1699000; Episodes: 6859\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1335.08\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 293.5181013216247 \n",
      "Num timesteps: 1700000; Episodes: 6862\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1335.70\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 262.52703242073375 \n",
      "Saving new model file...\n",
      "Num timesteps: 1701000; Episodes: 6864\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1341.52\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 168.10835885616666 \n",
      "Num timesteps: 1702000; Episodes: 6867\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1343.82\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 282.6675815968423 \n",
      "Num timesteps: 1703000; Episodes: 6870\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1330.25\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 270.7622617472713 \n",
      "Num timesteps: 1704000; Episodes: 6872\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1331.54\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 296.8515215824991 \n",
      "Num timesteps: 1705000; Episodes: 6875\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1332.79\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 287.2226610737587 \n",
      "Num timesteps: 1706000; Episodes: 6877\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1332.29\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 241.30128212116873 \n",
      "Num timesteps: 1707000; Episodes: 6880\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1332.45\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 193.09828333426506 \n",
      "Num timesteps: 1708000; Episodes: 6882\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1336.71\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.7577278184353 \n",
      "Num timesteps: 1709000; Episodes: 6885\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1334.68\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 150.843621341725 \n",
      "Num timesteps: 1710000; Episodes: 6887\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1335.32\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 287.7985778556317 \n",
      "Saving new model file...\n",
      "Num timesteps: 1711000; Episodes: 6889\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1335.40\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 279.9680652599448 \n",
      "Num timesteps: 1712000; Episodes: 6893\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1330.64\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 285.4664880148772 \n",
      "Num timesteps: 1713000; Episodes: 6896\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1322.38\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 270.78563748488284 \n",
      "Num timesteps: 1714000; Episodes: 6899\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1315.84\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 280.8215678331656 \n",
      "Num timesteps: 1715000; Episodes: 6902\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1303.67\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 291.6436941350155 \n",
      "Num timesteps: 1716000; Episodes: 6904\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1303.35\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 220.8047439527815 \n",
      "Num timesteps: 1717000; Episodes: 6907\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1303.16\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 222.33077971105212 \n",
      "Num timesteps: 1718000; Episodes: 6910\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1289.22\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 154.71802473360802 \n",
      "Num timesteps: 1719000; Episodes: 6912\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1285.41\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 214.61490521781693 \n",
      "Num timesteps: 1720000; Episodes: 6915\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1293.94\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 263.65921210230204 \n",
      "Saving new model file...\n",
      "Num timesteps: 1721000; Episodes: 6917\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1294.41\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 296.53850590820815 \n",
      "Num timesteps: 1722000; Episodes: 6920\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1281.54\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.42941939269326 \n",
      "Num timesteps: 1723000; Episodes: 6924\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1267.77\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 237.38719904967965 \n",
      "Num timesteps: 1724000; Episodes: 6927\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1258.70\n",
      "Num timesteps: 1725000; Episodes: 6929\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1258.47\n",
      "Num timesteps: 1726000; Episodes: 6931\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1259.09\n",
      "Num timesteps: 1727000; Episodes: 6934\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1253.40\n",
      "Num timesteps: 1728000; Episodes: 6938\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1241.10\n",
      "Num timesteps: 1729000; Episodes: 6940\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1240.49\n",
      "Num timesteps: 1730000; Episodes: 6943\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1253.09\n",
      "Saving new model file...\n",
      "Num timesteps: 1731000; Episodes: 6946\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1239.96\n",
      "Num timesteps: 1732000; Episodes: 6949\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1227.17\n",
      "Num timesteps: 1733000; Episodes: 6952\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1228.04\n",
      "Num timesteps: 1734000; Episodes: 6955\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1220.51\n",
      "Num timesteps: 1735000; Episodes: 6958\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1225.16\n",
      "Num timesteps: 1736000; Episodes: 6960\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1224.37\n",
      "Num timesteps: 1737000; Episodes: 6962\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1224.70\n",
      "Num timesteps: 1738000; Episodes: 6965\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1223.03\n",
      "Num timesteps: 1739000; Episodes: 6968\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1221.60\n",
      "Num timesteps: 1740000; Episodes: 6970\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1232.23\n",
      "Saving new model file...\n",
      "Num timesteps: 1741000; Episodes: 6973\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1232.80\n",
      "Num timesteps: 1742000; Episodes: 6976\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1227.01\n",
      "Num timesteps: 1743000; Episodes: 6979\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1214.18\n",
      "Num timesteps: 1744000; Episodes: 6981\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1221.62\n",
      "Num timesteps: 1745000; Episodes: 6984\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1224.84\n",
      "Num timesteps: 1746000; Episodes: 6987\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1214.99\n",
      "Num timesteps: 1747000; Episodes: 6990\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1206.61\n",
      "Num timesteps: 1748000; Episodes: 6993\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1212.32\n",
      "Num timesteps: 1749000; Episodes: 6995\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1222.70\n",
      "Num timesteps: 1750000; Episodes: 6997\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1223.10\n",
      "Saving new model file...\n",
      "Num timesteps: 1751000; Episodes: 7000\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1225.35\n",
      "Num timesteps: 1752000; Episodes: 7003\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1232.11\n",
      "Num timesteps: 1753000; Episodes: 7005\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1231.83\n",
      "Num timesteps: 1754000; Episodes: 7008\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1230.55\n",
      "Num timesteps: 1755000; Episodes: 7011\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1228.31\n",
      "Num timesteps: 1756000; Episodes: 7014\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1228.97\n",
      "Num timesteps: 1757000; Episodes: 7017\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1222.01\n",
      "Num timesteps: 1758000; Episodes: 7019\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1221.76\n",
      "Num timesteps: 1759000; Episodes: 7022\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1235.25\n",
      "Num timesteps: 1760000; Episodes: 7024\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1248.02\n",
      "Saving new model file...\n",
      "Num timesteps: 1761000; Episodes: 7026\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1252.84\n",
      "Num timesteps: 1762000; Episodes: 7028\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1256.57\n",
      "Num timesteps: 1763000; Episodes: 7031\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1255.96\n",
      "Num timesteps: 1764000; Episodes: 7033\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1251.72\n",
      "Num timesteps: 1765000; Episodes: 7036\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1257.73\n",
      "Num timesteps: 1766000; Episodes: 7038\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1265.16\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 173.1278509647872 \n",
      "Num timesteps: 1767000; Episodes: 7040\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1269.83\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 288.11079767550797 \n",
      "Num timesteps: 1768000; Episodes: 7043\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1260.80\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 273.4741451644775 \n",
      "Num timesteps: 1769000; Episodes: 7045\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1270.57\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 213.35340977644628 \n",
      "Num timesteps: 1770000; Episodes: 7047\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1282.83\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 229.60762805231016 \n",
      "Saving new model file...\n",
      "Num timesteps: 1771000; Episodes: 7049\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1290.94\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 280.80873556272786 \n",
      "Num timesteps: 1772000; Episodes: 7052\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1282.53\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 252.1575613592947 \n",
      "Num timesteps: 1773000; Episodes: 7054\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1292.78\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 182.82230590684188 \n",
      "Num timesteps: 1774000; Episodes: 7057\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1296.21\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 275.4801337576884 \n",
      "Num timesteps: 1775000; Episodes: 7061\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1273.94\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 223.50355278313734 \n",
      "Num timesteps: 1776000; Episodes: 7064\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1279.12\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 230.93886956018534 \n",
      "Num timesteps: 1777000; Episodes: 7066\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1279.20\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 253.8215879437672 \n",
      "Num timesteps: 1778000; Episodes: 7068\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1274.99\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 273.13723032128183 \n",
      "Num timesteps: 1779000; Episodes: 7071\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1274.91\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 274.41093289819787 \n",
      "Num timesteps: 1780000; Episodes: 7073\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1274.55\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 292.1094470505846 \n",
      "Saving new model file...\n",
      "Num timesteps: 1781000; Episodes: 7075\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1277.53\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 271.774822886673 \n",
      "Num timesteps: 1782000; Episodes: 7079\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1279.13\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 282.2382444035406 \n",
      "Num timesteps: 1783000; Episodes: 7081\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1278.86\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 242.0722960382887 \n",
      "Num timesteps: 1784000; Episodes: 7084\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1278.42\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 203.40130768986302 \n",
      "Num timesteps: 1785000; Episodes: 7086\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1288.23\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.1089665044256 \n",
      "Num timesteps: 1786000; Episodes: 7088\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1283.87\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 246.42172286017995 \n",
      "Num timesteps: 1787000; Episodes: 7091\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1290.82\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 282.5332125031948 \n",
      "Num timesteps: 1788000; Episodes: 7094\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1290.22\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.19601121736616 \n",
      "Num timesteps: 1789000; Episodes: 7096\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1291.23\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 289.12424432538137 \n",
      "Num timesteps: 1790000; Episodes: 7099\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1299.68\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 282.80071253979725 \n",
      "Saving new model file...\n",
      "Num timesteps: 1791000; Episodes: 7101\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1293.01\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 289.4189469702588 \n",
      "Num timesteps: 1792000; Episodes: 7104\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1293.83\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 267.0799352147416 \n",
      "Num timesteps: 1793000; Episodes: 7108\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1269.98\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 210.99873312987893 \n",
      "Num timesteps: 1794000; Episodes: 7111\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1279.60\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 246.95267682005505 \n",
      "Num timesteps: 1795000; Episodes: 7113\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1279.28\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 268.1759488593475 \n",
      "Num timesteps: 1796000; Episodes: 7116\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1275.85\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 294.3589348922509 \n",
      "Num timesteps: 1797000; Episodes: 7118\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1275.87\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 198.54275673336943 \n",
      "Num timesteps: 1798000; Episodes: 7121\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1275.47\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 158.1156100377292 \n",
      "Num timesteps: 1799000; Episodes: 7124\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1265.17\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 298.0662392499582 \n",
      "Num timesteps: 1800000; Episodes: 7126\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1264.88\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 278.1356282576609 \n",
      "Saving new model file...\n",
      "Num timesteps: 1801000; Episodes: 7129\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1260.96\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 231.04304482399894 \n",
      "Num timesteps: 1802000; Episodes: 7131\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1261.07\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 276.2436186874765 \n",
      "Num timesteps: 1803000; Episodes: 7135\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1245.37\n",
      "Num timesteps: 1804000; Episodes: 7137\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1250.74\n",
      "Num timesteps: 1805000; Episodes: 7140\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1245.16\n",
      "Num timesteps: 1806000; Episodes: 7143\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1240.67\n",
      "Num timesteps: 1807000; Episodes: 7145\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1241.38\n",
      "Num timesteps: 1808000; Episodes: 7148\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1228.79\n",
      "Num timesteps: 1809000; Episodes: 7151\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1231.04\n",
      "Num timesteps: 1810000; Episodes: 7153\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1238.01\n",
      "Saving new model file...\n",
      "Num timesteps: 1811000; Episodes: 7156\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1243.25\n",
      "Num timesteps: 1812000; Episodes: 7158\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1239.19\n",
      "Num timesteps: 1813000; Episodes: 7161\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1250.59\n",
      "Num timesteps: 1814000; Episodes: 7165\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1227.27\n",
      "Num timesteps: 1815000; Episodes: 7168\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1237.00\n",
      "Num timesteps: 1816000; Episodes: 7170\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1236.78\n",
      "Num timesteps: 1817000; Episodes: 7173\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1231.24\n",
      "Num timesteps: 1818000; Episodes: 7176\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1221.11\n",
      "Num timesteps: 1819000; Episodes: 7178\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1230.23\n",
      "Num timesteps: 1820000; Episodes: 7181\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1237.99\n",
      "Saving new model file...\n",
      "Num timesteps: 1821000; Episodes: 7183\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1237.53\n",
      "Num timesteps: 1822000; Episodes: 7186\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1228.70\n",
      "Num timesteps: 1823000; Episodes: 7189\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1226.96\n",
      "Num timesteps: 1824000; Episodes: 7192\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1234.29\n",
      "Num timesteps: 1825000; Episodes: 7194\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1234.13\n",
      "Num timesteps: 1826000; Episodes: 7197\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1234.09\n",
      "Num timesteps: 1827000; Episodes: 7199\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1232.92\n",
      "Num timesteps: 1828000; Episodes: 7201\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1232.52\n",
      "Num timesteps: 1829000; Episodes: 7204\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1233.63\n",
      "Num timesteps: 1830000; Episodes: 7206\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1240.54\n",
      "Saving new model file...\n",
      "Num timesteps: 1831000; Episodes: 7209\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1245.29\n",
      "Num timesteps: 1832000; Episodes: 7211\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1255.58\n",
      "Num timesteps: 1833000; Episodes: 7214\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1251.32\n",
      "Num timesteps: 1834000; Episodes: 7217\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1250.33\n",
      "Num timesteps: 1835000; Episodes: 7220\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1251.20\n",
      "Num timesteps: 1836000; Episodes: 7222\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1251.20\n",
      "Num timesteps: 1837000; Episodes: 7224\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1261.61\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 296.9837898664558 \n",
      "Num timesteps: 1838000; Episodes: 7227\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1263.82\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 271.1839819993653 \n",
      "Num timesteps: 1839000; Episodes: 7230\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1257.66\n",
      "Num timesteps: 1840000; Episodes: 7233\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1248.96\n",
      "Saving new model file...\n",
      "Num timesteps: 1841000; Episodes: 7237\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1242.95\n",
      "Num timesteps: 1842000; Episodes: 7239\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1244.74\n",
      "Num timesteps: 1843000; Episodes: 7241\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1256.15\n",
      "Num timesteps: 1844000; Episodes: 7244\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1252.64\n",
      "Num timesteps: 1845000; Episodes: 7246\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1250.09\n",
      "Num timesteps: 1846000; Episodes: 7248\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1262.42\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 277.0134712210003 \n",
      "Num timesteps: 1847000; Episodes: 7251\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1262.76\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 270.08153520962793 \n",
      "Num timesteps: 1848000; Episodes: 7254\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1253.45\n",
      "Num timesteps: 1849000; Episodes: 7257\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1250.44\n",
      "Num timesteps: 1850000; Episodes: 7259\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1253.98\n",
      "Saving new model file...\n",
      "Num timesteps: 1851000; Episodes: 7262\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1258.82\n",
      "Num timesteps: 1852000; Episodes: 7264\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1268.85\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 208.0845200926972 \n",
      "Num timesteps: 1853000; Episodes: 7266\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1282.13\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 266.4306513277512 \n",
      "Num timesteps: 1854000; Episodes: 7269\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1273.74\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 290.61679409021565 \n",
      "Num timesteps: 1855000; Episodes: 7272\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1272.83\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 299.96210362102846 \n",
      "Num timesteps: 1856000; Episodes: 7274\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1279.26\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 225.32133343006012 \n",
      "Num timesteps: 1857000; Episodes: 7277\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1279.16\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 296.6865405443649 \n",
      "Num timesteps: 1858000; Episodes: 7280\n",
      "Best mean reward: 1345.31 - Last mean reward per episode: 1269.34\n",
      "\u001b[0;33mReward threshold achieved\u001b[0m\n",
      "Evaluating model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 100 Episodes: 300.275079752431 \n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalkerHardcore-model/best_model...\n",
      "Saving training info...\n",
      "./models/PMTG_SAC-bipedalWalkerHardcore-model/best_single/2022-10-13_08-09-02/training_info.txt\n",
      "Saving reward CSV-data...\n",
      "MISSION COMPLETED\n",
      "Score: 300.275079752431+/-22.259018349929633 reached at Episode: 7280 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x7f25d7e36fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(n_timesteps, tb_log_name=tb_log_name, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show reward plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'bipedWalker2d PMTG(CPG+SAC) rew1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 4), facecolor=\"white\")\n",
    "plt.grid()\n",
    "plt.plot(callback.s_timestep, callback.s_reward, 'k')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"bipedWalker2d PMTG(CPG+SAC) rew1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38513d5fc8944757278d1305d220fcdba7cc001081561143afc16cb99cb5ffdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
