{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoggi/biped_research/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import ode\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch as th\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import TD3, SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "from bipedalWalkerPMTG import BipedalWalkerPMTG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to use gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "model_saved_file = \"./models/bipedWalker_velctrl_PMTG_SAC\"\n",
    "log_dir = \"./models/PMTG_SAC-bipedalWalker2d_velctrl-model\"\n",
    "best_model_dir = \"./models/PMTG_SAC-bipedalWalker2d_velctrl-model/best_single\"\n",
    "video_prefix = \"bipedWalker_velctrl_PMTG_SAC\"\n",
    "csv_addr = \"./statistics/bipedWalker2d_velctrl_PMTG_SAC_rewards.csv\"\n",
    "time_addr = \"./statistics/bipedWalker2d_velctrl_PMTG_SAC_elapcedTime.txt\"\n",
    "csv_path = \"./statistics/bipedWalker2d_velctrl_PMTG_SAC_rewards.csv\"\n",
    "time_path = \"./statistics/bipedWalker2d_velctrl_PMTG_SAC_elapcedTime.txt\"\n",
    "tb_log = \"./PMTG_velctrl_tensorboard/\"\n",
    "tb_log_name = \"PMTG_SAC_velctrl\"\n",
    "\n",
    "# Training Parameters\n",
    "if use_gpu == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    num_cpu = 1\n",
    "\n",
    "is_hard = False\n",
    "n_timesteps = int(1e6)\n",
    "seed = 42\n",
    "check_freq = 1000\n",
    "reward_threshold = 300\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 7.3e-4\n",
    "batch_size = 256\n",
    "gamma = 0.98\n",
    "tau = 0.02\n",
    "buffer_size = 300000\n",
    "learning_starts = 10000\n",
    "noise_std = 0.1\n",
    "gradient_steps = 1\n",
    "train_freq = 1\n",
    "n_layers = 2\n",
    "n_neurons = 256\n",
    "ent_coef = 'auto'\n",
    "log_std_init = -3\n",
    "\n",
    "activation = th.nn.modules.activation.ReLU\n",
    "net_arch = [n_neurons]*n_layers\n",
    "\n",
    "policy_kwargs = dict(activation_fn=activation, \n",
    "                    net_arch=net_arch)\n",
    "\n",
    "hyperparameters = {\n",
    "        #\"env_name\": env_name,\n",
    "        \"n_timesteps\": n_timesteps,\n",
    "        \"seed\": seed,\n",
    "        \"noise_std\": noise_std,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"tau\": tau,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"train_freq\": train_freq,\n",
    "\n",
    "        \"policy_kwargs\": dict(\n",
    "            net_arch=net_arch,\n",
    "            log_std_init=log_std_init,\n",
    "            activation_fn=activation,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# other params\n",
    "it = 0\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BipedalWalkerPMTG(\n",
    "    is_hard=is_hard, \n",
    "    action_repeat=1, \n",
    "    act_noise=0.3, \n",
    "    rew_scale=1.0,\n",
    "    learn=True,\n",
    "    vel_ctrl=True\n",
    "    )\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_env = BipedalWalkerPMTG(\n",
    "    is_hard=is_hard, \n",
    "    action_repeat=1, \n",
    "    act_noise=0.0, \n",
    "    rew_scale=1.0,\n",
    "    learn=False,\n",
    "    vel_ctrl=True\n",
    "    )\n",
    "\n",
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(policy='MlpPolicy', \n",
    "        env=env,\n",
    "        # action_noise=action_noise,\n",
    "        train_freq=train_freq,\n",
    "\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        tau=tau,\n",
    "        gamma=gamma,\n",
    "        buffer_size=buffer_size,\n",
    "        learning_starts=learning_starts,\n",
    "        gradient_steps=gradient_steps,\n",
    "        ent_coef=ent_coef,\n",
    "\n",
    "        policy_kwargs=policy_kwargs,\n",
    "\n",
    "        device=device,\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        tensorboard_log=tb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed\n",
      "_XSERVTransMakeAllCOTSServerListeners: server already running\n",
      "(EE) \n",
      "Fatal server error:\n",
      "(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \n"
     ]
    }
   ],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.mean_reward_lst = []\n",
    "        self.timesteps = []\n",
    "        self.s_reward = []\n",
    "        self.s_timestep = []\n",
    "        self.file_number = 0\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          \n",
    "          if len(x) > 0:\n",
    "            # Mean training reward over the last 100 episodes\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "            self.mean_reward_lst.append(mean_reward)\n",
    "            self.timesteps.append(self.num_timesteps)\n",
    "            episodes = len(y)\n",
    "\n",
    "            self.s_reward.append(mean_reward)\n",
    "            self.s_timestep.append(self.num_timesteps)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "              print(f\"Num timesteps: {self.num_timesteps}; Episodes: {episodes}\")\n",
    "              print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "              self.best_mean_reward = mean_reward\n",
    "              print(f\"Saving new best model to {self.save_path}...\")\n",
    "              self.model.save(self.save_path)\n",
    "\n",
    "        if self.num_timesteps % 10000 == 0:\n",
    "          print(\"Saving new model file...\")\n",
    "          self.file_number += 1\n",
    "          filename = \"model{0}\".format(self.file_number)\n",
    "          nsave_path = os.path.join(log_dir, filename)\n",
    "          self.model.save(nsave_path)\n",
    "          \n",
    "        return True\n",
    "\n",
    "    def get_mean_reward(self):\n",
    "      return self.mean_reward_lst, self.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000; Episodes: 113\n",
      "Best mean reward: -inf - Last mean reward per episode: 259.92\n",
      "Saving new best model to ./models/PMTG_SAC-bipedalWalker2d-model/best_model...\n",
      "Num timesteps: 2000; Episodes: 125\n",
      "Best mean reward: 259.92 - Last mean reward per episode: 222.91\n",
      "Num timesteps: 3000; Episodes: 138\n",
      "Best mean reward: 259.92 - Last mean reward per episode: 182.71\n",
      "Num timesteps: 4000; Episodes: 151\n",
      "Best mean reward: 259.92 - Last mean reward per episode: 142.41\n",
      "Num timesteps: 5000; Episodes: 165\n",
      "Best mean reward: 259.92 - Last mean reward per episode: 99.16\n",
      "Num timesteps: 6000; Episodes: 179\n",
      "Best mean reward: 259.92 - Last mean reward per episode: 57.78\n",
      "Num timesteps: 7000; Episodes: 194\n",
      "Best mean reward: 259.92 - Last mean reward per episode: 11.51\n",
      "Num timesteps: 8000; Episodes: 209\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.29\n",
      "Num timesteps: 9000; Episodes: 222\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.29\n",
      "Num timesteps: 10000; Episodes: 235\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.35\n",
      "Saving new model file...\n",
      "Num timesteps: 11000; Episodes: 251\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.11\n",
      "Num timesteps: 12000; Episodes: 259\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -9.80\n",
      "Num timesteps: 13000; Episodes: 261\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.10\n",
      "Num timesteps: 14000; Episodes: 265\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.41\n",
      "Num timesteps: 15000; Episodes: 269\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.32\n",
      "Num timesteps: 16000; Episodes: 276\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.49\n",
      "Num timesteps: 17000; Episodes: 280\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -10.51\n",
      "Num timesteps: 18000; Episodes: 291\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -11.22\n",
      "Num timesteps: 19000; Episodes: 294\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -11.48\n",
      "Num timesteps: 20000; Episodes: 295\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -11.66\n",
      "Saving new model file...\n",
      "Num timesteps: 21000; Episodes: 296\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -12.35\n",
      "Num timesteps: 22000; Episodes: 296\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -12.35\n",
      "Num timesteps: 23000; Episodes: 297\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -12.93\n",
      "Num timesteps: 24000; Episodes: 298\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -12.97\n",
      "Num timesteps: 25000; Episodes: 299\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -13.41\n",
      "Num timesteps: 26000; Episodes: 302\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -13.42\n",
      "Num timesteps: 27000; Episodes: 302\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -13.42\n",
      "Num timesteps: 28000; Episodes: 304\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -14.32\n",
      "Num timesteps: 29000; Episodes: 305\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -14.26\n",
      "Num timesteps: 30000; Episodes: 307\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -14.18\n",
      "Saving new model file...\n",
      "Num timesteps: 31000; Episodes: 307\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -14.18\n",
      "Num timesteps: 32000; Episodes: 311\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -14.32\n",
      "Num timesteps: 33000; Episodes: 316\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -15.28\n",
      "Num timesteps: 34000; Episodes: 324\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -16.02\n",
      "Num timesteps: 35000; Episodes: 327\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -16.69\n",
      "Num timesteps: 36000; Episodes: 333\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -17.21\n",
      "Num timesteps: 37000; Episodes: 338\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -17.78\n",
      "Num timesteps: 38000; Episodes: 341\n",
      "Best mean reward: 259.92 - Last mean reward per episode: -18.40\n"
     ]
    }
   ],
   "source": [
    "model.learn(n_timesteps, tb_log_name=tb_log_name, callback=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38513d5fc8944757278d1305d220fcdba7cc001081561143afc16cb99cb5ffdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
